{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataaugmentation\n",
    "\n",
    "## Version 1.0\n",
    "\n",
    "Has a Torchvision and a Albumentations part\n",
    "\n",
    "The Torchvision part wasnt working, therefore the albumentations part was added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "use_torchvision = False\n",
    "\n",
    "if use_torchvision:\n",
    "    import torch\n",
    "    import torch.utils.data\n",
    "    import torchvision\n",
    "    from torchvision import models, datasets, tv_tensors\n",
    "    from torchvision.transforms import v2\n",
    "    from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision\n",
    "\n",
    "(Doesnt Work in the model!!)\n",
    "\n",
    "Source: https://pytorch.org/vision/stable/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_torchvision:\n",
    "    os.path.exists(\"../../../old/BAA/Data/train/\")\n",
    "\n",
    "    PATH_IMAGES = \"../../../old/BAA/Data/train/\"\n",
    "    PATH_ANNOTATIONS = \"../../../old/BAA/Data/train/coco_train.json\"\n",
    "    TEST_FILE = \"../Test/\"\n",
    "\n",
    "    transforms = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            v2.RandomPhotometricDistort(p=1),\n",
    "            v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n",
    "            v2.RandomIoUCrop(),\n",
    "            v2.RandomHorizontalFlip(p=1),\n",
    "            v2.SanitizeBoundingBoxes(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.CocoDetection(PATH_IMAGES, PATH_ANNOTATIONS, transforms=transforms)\n",
    "\n",
    "    dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=\"all\")\n",
    "    #dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=\"all\")\n",
    "\n",
    "    sample = dataset[0]\n",
    "    img, target = sample\n",
    "    print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "    print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")\n",
    "    print(sample)\n",
    "    class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "        def __init__(self, img_folder, processor, image_name, transforms=None):\n",
    "            ann_file = os.path.join(img_folder, image_name)\n",
    "            super(CocoDetection, self).__init__(img_folder, ann_file, transforms)\n",
    "            self.processor = processor\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # read in PIL image and target in COCO format\n",
    "            # feel free to add data augmentation here before passing them to the next step\n",
    "            \n",
    "            \n",
    "            img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "            # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "            image_id = self.ids[idx]\n",
    "            target = {'image_id': image_id, 'annotations': target}\n",
    "            encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "            pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "            target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "            return pixel_values, target\n",
    "    dataset = datasets.CocoDetection(PATH_IMAGES, PATH_ANNOTATIONS)\n",
    "\n",
    "    sample = dataset[0]\n",
    "    img, target = sample\n",
    "    print(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")\n",
    "    print(sample)\n",
    "    transforms = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            v2.RandomPhotometricDistort(p=1),\n",
    "            v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n",
    "            v2.RandomIoUCrop(),\n",
    "            v2.RandomHorizontalFlip(p=1),\n",
    "            v2.SanitizeBoundingBoxes(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.CocoDetection(PATH_IMAGES, PATH_ANNOTATIONS, transforms=transforms)\n",
    "    dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=[\"boxes\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Augmented and normal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_torchvision:\n",
    "\n",
    "    transforms = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            v2.RandomPhotometricDistort(),\n",
    "            v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n",
    "            v2.RandomIoUCrop(),\n",
    "            v2.RandomHorizontalFlip(),\n",
    "            v2.SanitizeBoundingBoxes(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ]\n",
    "    )\n",
    "    dataset_original = datasets.CocoDetection(PATH_IMAGES, PATH_ANNOTATIONS)\n",
    "    dataset_aug = datasets.CocoDetection(PATH_IMAGES, PATH_ANNOTATIONS, transforms=transforms)\n",
    "    dataset_original = datasets.wrap_dataset_for_transforms_v2(dataset_original, target_keys=(\"boxes\", \"labels\"))\n",
    "    dataset_aug = datasets.wrap_dataset_for_transforms_v2(dataset_aug, target_keys=(\"boxes\", \"labels\"))\n",
    "    dataset_combined = torch.utils.data.ConcatDataset([dataset_original, dataset_aug])\n",
    "    img, tar = dataset_combined[5]\n",
    "    # img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albumnentations\n",
    "\n",
    "Source: https://albumentations.ai/docs/getting_started/installation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to visualize the augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35,\n",
    "        color=TEXT_COLOR,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image and Annotations + define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IMAGES = \"../../../old/BAA/Data/train/\"\n",
    "PATH_ANNOTATIONS = \"../../../old/BAA/Data/train/coco_train.json\"\n",
    "TEST_FILE = \"../Test/\"\n",
    "image_name = 'LB00000207.png'\n",
    "image = cv2.imread(PATH_IMAGES + image_name)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "with open(PATH_ANNOTATIONS, \"r\") as f:\n",
    "    coco_mini = json.load(f)\n",
    "category_id_to_name = {0:\"Bildartefakte\", 1:\"Manuell Überprüfter Container\", 2:\"Schimmel\", 3:\"Schmutz\", 4:\"Schäden\", 5:\"Schädlinge\", 6:\"Sonstiges\", 7:\"Starke Überbeleuchtung oder Unterbeleuchtung\", 8:\"Unerwartetes Layout\", 9:\"Wasser\"}\n",
    "\n",
    "img_id = -1\n",
    "for n in range(len(coco_mini[\"images\"])):\n",
    "\tif coco_mini[\"images\"][n][\"file_name\"] == image_name:\n",
    "\t\timg_id = coco_mini[\"images\"][n][\"id\"]\n",
    "\n",
    "annotations = []\n",
    "bboxes = []\n",
    "category_id = []\n",
    "for n in range(len(coco_mini[\"annotations\"])):\n",
    "\tif coco_mini[\"annotations\"][n][\"image_id\"] == img_id:\n",
    "\t\tannotations.append(coco_mini[\"annotations\"][n])\n",
    "\t\tbboxes.append(coco_mini[\"annotations\"][n][\"bbox\"])\n",
    "\t\tcategory_id.append(coco_mini[\"annotations\"][n][\"category_id\"])\n",
    "\n",
    "print(img_id)\n",
    "print(annotations)\n",
    "print(bboxes)\n",
    "print(category_id)\n",
    "\n",
    "for i in zip(annotations, bboxes, category_id):\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use and visualize Augmentation of the loaded Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transform(image=image, bboxes=bboxes, category_ids=category_id)\n",
    "\n",
    "visualize(\n",
    "    transformed['image'],\n",
    "    transformed['bboxes'],\n",
    "    transformed['category_ids'],\n",
    "    category_id_to_name,\n",
    ")\n",
    "print(transformed)\n",
    "print(transformed[\"image\"].shape)\n",
    "print(list(transformed[\"bboxes\"][0]))\n",
    "print(transformed[\"bboxes\"][0][2] * transformed[\"bboxes\"][0][3])\n",
    "PATH_SAVE = \"../../../old/BAA/Data/train_aug/\"\n",
    "img_name = \"testimage.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image\n",
    "im = Image.fromarray(transformed[\"image\"])\n",
    "im.save(os.path.join(PATH_SAVE, img_name), \"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Production: Data (image) augmentation for a whole directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(path_to_images, path_to_save, path_to_annotations, new_name_addition = \"A\"):\n",
    "\t# dataload\n",
    "\timage_list = [os.path.join(path_to_images, filename) for filename in os.listdir(path_to_images) if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "\twith open(path_to_annotations, \"r\") as f:\n",
    "\t\tcoco_data = json.load(f)\n",
    "\n",
    "\thighest_img_id = coco_data[\"images\"][-1][\"id\"]\n",
    "\thighest_ann_id = coco_data[\"annotations\"][-1][\"id\"]\n",
    "\n",
    "\t# prep output\n",
    "\tdef createSubsetStructure(originalData, description):\n",
    "\t\tsubsetStructure = {\n",
    "\t\t\t\"info\": [{\"year\": int(datetime.date.today().year)},\n",
    "\t\t\t\t\t{\"version\": \"1.0\"},\n",
    "\t\t\t\t\t{\"description\": description},\n",
    "\t\t\t\t\t{\"contributer\": \"Michael Infanger\"},\n",
    "\t\t\t\t\t{\"url\": \"\"},\n",
    "\t\t\t\t\t{\"date_created\": str(datetime.datetime.now())}],\n",
    "\t\t\t\"categories\": originalData[\"categories\"],\n",
    "\t\t\t\"images\": [],\n",
    "\t\t\t\"annotations\": []\n",
    "\t\t}\n",
    "\t\treturn subsetStructure\n",
    "\n",
    "\tdef writeJson(jsonObject, filename):\n",
    "\t\tif filename.split(\".\")[-1] != \"json\":\n",
    "\t\t\tfilename = f\"{filename}.json\"\n",
    "\t\twith open(filename, \"w\") as g:\n",
    "\t\t\tg.write(jsonObject)\n",
    "\n",
    "\tnew_coco_data = createSubsetStructure(coco_data, \"Augmented_data_\" + new_name_addition)\n",
    "\n",
    "\t# Augmentationpipeline\n",
    "\ttransform = A.Compose([\n",
    "\t\t\tA.HorizontalFlip(p=0.6),\n",
    "\t\t\tA.ShiftScaleRotate(p=0.6),\n",
    "\t\t\tA.RandomBrightnessContrast(p=0.4),\n",
    "\t\t\tA.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3),\n",
    "\t\t],\n",
    "\t\tbbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
    "\t)\n",
    "\n",
    "\t# category_id_to_name = {0:\"Bildartefakte\", 1:\"Manuell Überprüfter Container\", 2:\"Schimmel\", 3:\"Schmutz\", 4:\"Schäden\", 5:\"Schädlinge\", 6:\"Sonstiges\", 7:\"Starke Überbeleuchtung oder Unterbeleuchtung\", 8:\"Unerwartetes Layout\", 9:\"Wasser\"}\n",
    "\n",
    "\tfor image_path in image_list:\n",
    "\t\timage_name = image_path.split(\"/\")[-1]\n",
    "\t\timage = cv2.imread(image_path)\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\t\t# dataprep\n",
    "\t\timg_id = -1\n",
    "\t\tfor n in range(len(coco_data[\"images\"])):\n",
    "\t\t\tif coco_data[\"images\"][n][\"file_name\"] == image_name:\n",
    "\t\t\t\timg_id = coco_data[\"images\"][n][\"id\"]\n",
    "\n",
    "\t\tannotations = []\n",
    "\t\tbboxes = []\n",
    "\t\tcategory_id = []\n",
    "\t\tfor n in range(len(coco_data[\"annotations\"])):\n",
    "\t\t\tif coco_data[\"annotations\"][n][\"image_id\"] == img_id:\n",
    "\t\t\t\tannotations.append(coco_data[\"annotations\"][n])\n",
    "\t\t\t\tbboxes.append(coco_data[\"annotations\"][n][\"bbox\"])\n",
    "\t\t\t\tcategory_id.append(coco_data[\"annotations\"][n][\"category_id\"])\n",
    "\n",
    "\t\t# run data augmentation\n",
    "\t\ttransformed = transform(image=image, bboxes=bboxes, category_ids=category_id)\n",
    "\t\talt_img_name = new_name_addition + image_name\n",
    "\n",
    "\n",
    "\t\t# prepare data to save in coco format\n",
    "\t\thighest_img_id += 1\n",
    "\t\timage_info = {\n",
    "\t\t\t\"width\": transformed[\"image\"].shape[1],\n",
    "            \"height\": transformed[\"image\"].shape[0],\n",
    "            \"id\": highest_img_id,\n",
    "            \"file_name\": alt_img_name\n",
    "\t\t\t}\n",
    "\n",
    "\t\tnew_coco_data[\"images\"].append(image_info)\n",
    "\n",
    "\t\tfor i in zip(transformed[\"bboxes\"], transformed[\"category_ids\"]):\n",
    "\t\t\thighest_ann_id += 1\n",
    "\t\t\tarea_new = i[0][2] * i[0][3]\n",
    "\t\t\tannotation = {\n",
    "\t\t\t\t\"id\": highest_ann_id,\n",
    "\t\t\t\t\"image_id\": highest_img_id,\n",
    "\t\t\t\t\"category_id\": i[1],\n",
    "\t\t\t\t\"segmentation\": [],\n",
    "\t\t\t\t\"bbox\": list(i[0]),\n",
    "\t\t\t\t\"ignore\": 0,\n",
    "\t\t\t\t\"iscrowd\": 0,\n",
    "\t\t\t\t\"area\": area_new\n",
    "        \t}\n",
    "\t\t\tnew_coco_data[\"annotations\"].append(annotation)\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\tim = Image.fromarray(transformed[\"image\"])\n",
    "\t\tim.save(os.path.join(path_to_save, alt_img_name), \"PNG\")\n",
    "\n",
    "\tjson_file = json.dumps(new_coco_data, indent=4)\n",
    "\twriteJson(json_file, path_to_save +\"Augmented_\" + new_name_addition + \"_\" + path_to_annotations.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Data (image) augmentation\n",
    "\n",
    "imports are needed\n",
    "\n",
    "writes files directly to the new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "# Uncomment to run the function above\n",
    "# data_augmentation(\"../../../old/BAA/Data/val_max/\", \"../../../old/BAA/Data/val_aug/\", \"../../../old/BAA/Data/val_max/coco_val_max.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
