{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Assessment für die Bachelorarbeit: Schadenserkennung bei Büchern mit machine learning\n",
    "\n",
    "## Version 1.0\n",
    "\n",
    "Besteht aus einem YOLO und COCO Teil. \n",
    "Für den Datenexport sollte COCO verwendet werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einlesen der Daten von json\n",
    "data = pd.read_json(\"../../../old/BAA/Annotierte_Bilder/2024-02-21-Schadenserkennung-Annotationen.json\")\n",
    "data = data.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verwerfen von nicht brauchbaren Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verwerfen von nicht zu verwendenen Daten\n",
    "#\n",
    "#   created_at und updated_at geben keine informationen über den inhalt des bildes bzw der annotation\n",
    "#   lead_time interessiert nicht\n",
    "#   annotator_id ist die id + 54\n",
    "#   annotator ist immer 1\n",
    "#   choice hat 974/1081 nullwerte (Nan)\n",
    "#   \n",
    "# Die restlichen Werte können relevant sein für das trainieren des algorithmus\n",
    "\n",
    "data_dropped = data.drop([\"created_at\", \"updated_at\", \"lead_time\", \"annotation_id\", \"annotator\", \"choice\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategorie \"label\" weiter aufteilen\n",
    "Extrahieren der einzelnen Schadensarten und festlegen ob es überhaupt schäden hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.json_normalize(data_dropped[\"label\"])\n",
    "total = ['Schimmel', 'Wasser', 'Sonstiges', 'Schäden', 'Schmutz', 'Schädlinge']\n",
    "\n",
    "# Add column for every category used\n",
    "for x in total:\n",
    "    data_dropped[x] = False\n",
    "data_dropped[\"hat_schäden\"] = None\n",
    "\n",
    "# True where category is true\n",
    "# rows\n",
    "for x in range(label.shape[0]):\n",
    "    rowx = set()\n",
    "    # colums\n",
    "    for y in range(label.shape[1]):\n",
    "        if label[y][x] != None:\n",
    "            rowx.add(label[y][x][\"polygonlabels\"][0])\n",
    "    if label[0][x] == None:\n",
    "        data_dropped.loc[x+1, \"hat_schäden\"]= False\n",
    "    else:\n",
    "        for s in rowx:\n",
    "            data_dropped.loc[x+1, s] = True\n",
    "        data_dropped.loc[x+1, \"hat_schäden\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datentyps kontrollieren\n",
    "image ist ein objekt, da strings in pandas so gespeichert werden.\n",
    "\n",
    "label ist ein objekt da es gemischte Datentypen enthält: strings, int, float, bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"hat_schäden\" da es nur True und Fale drin hat\n",
    "data_preprocessed = data_dropped.astype({\"hat_schäden\" : \"bool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auf None/Nan kontrollieren\n",
    "Wenn diese in \"image\" auftreten --> löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False = keine None/Nan\n",
    "nonetest = data_preprocessed[\"image\"].isnull().any()\n",
    "\n",
    "if nonetest == True:\n",
    "    # drop rows with None only\n",
    "    data_preprocessed.dropna(axis=0, how=\"all\", inplace=True)\n",
    "    # drop rows with none in image, without areference it is impossible to get to the right picture\n",
    "    data_preprocessed.dropna(axis=0, subset=[\"image\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mengen und verschiedene Anhaltspunkte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_schäden = data_preprocessed[\"hat_schäden\"].value_counts()\n",
    "print(dict(hat_schäden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es hat 882 Einträge die schäden aufweisen und 199 die keine haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthlist = {}\n",
    "for x in total:\n",
    "    truthlist[x] = data_preprocessed[x].value_counts()[True]\n",
    "print(truthlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr grosse unterschiede in den Mengen der Daten, viel schmutz und viel sonstiges aber wenige Schimmel und Schädlingsfälle\n",
    "In 871 von 882 Bildern mit beschädigungen sind Schmutzschäden drin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthlist.update({\"hat_schäden\":hat_schäden[True], \"ohne_schäden\": hat_schäden[False]})\n",
    "\n",
    "# Plot of the values\n",
    "\n",
    "labels = list(truthlist.keys())\n",
    "values = list(truthlist.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))  \n",
    "bars = plt.bar(labels, values, color='skyblue')\n",
    "plt.xlabel('Kategorien')\n",
    "plt.ylabel('Anzahl Bilder')\n",
    "plt.title('Schadensverteilung')\n",
    "plt.xticks(rotation=45)  \n",
    "# Add value on top of bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, int(yval), va='bottom', ha=\"center\")\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took the label value from: Kategorie weiter aufteilen\n",
    "# Displays the total amount of as damaged instances\n",
    "label.notnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrahieren des image namens und droppen von image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_namen_liste = []\n",
    "data_preprocessed[\"image_name\"] = None\n",
    "for i, row in data_preprocessed.iterrows():\n",
    "    data_preprocessed.loc[i, \"image_name\"] = row[\"image\"].split(\"/\")[-1]\n",
    "    z = row[\"image\"].split(\"/\")[-1]\n",
    "    image_namen_liste.append(f\"{z}\")\n",
    "data_preprocessed = data_preprocessed.drop(\"image\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koordinate in Listen zusammenfassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label = pd.json_normalize(data_preprocessed[\"label\"])\n",
    "\n",
    "data_preprocessed[\"boxpoints\"] = None\n",
    "for x in range(label.shape[0]):\n",
    "    info = {}\n",
    "    # colums\n",
    "    for y in range(label.shape[1]):\n",
    "        \n",
    "        temp = []\n",
    "        if label[y][x] != None:\n",
    "            temp.extend(label[y][x][\"polygonlabels\"])\n",
    "            temp.append(label[y][x][\"points\"])\n",
    "            info.update({y:temp})\n",
    "    info = np.asarray(info)\n",
    "    print(data_preprocessed[\"hat_schäden\"][46])\n",
    "    if data_preprocessed[\"hat_schäden\"][x+1]:\n",
    "        data_preprocessed.loc[x + 1, \"boxpoints\"] = info\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrahieren eines kleinen Datensets für erste Experimente\n",
    "\n",
    "Das Ziel ist es ein möglichst ausgeglichenes Datenset zu erhalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_sample_temp = []\n",
    "random_state = 42\n",
    "sample_size = 7\n",
    "for x in total:\n",
    "    size = len(data_preprocessed.loc[data_preprocessed[x] == True].index)\n",
    "    if size < sample_size:\n",
    "        data_sample_temp.append(data_preprocessed.loc[data_preprocessed[x] == True].sample(n=size, random_state=random_state).index)\n",
    "        print(\"Sample size for \" + x + \" is \" + str(size) + \" instead of the wanted sample size of \" + str(sample_size) + \". Has not more entries then that.\")\n",
    "    else:\n",
    "        data_sample_temp.append(data_preprocessed.loc[data_preprocessed[x] == True].sample(n=sample_size, random_state=random_state).index)\n",
    "\n",
    "# Takes a sample from not damaged bookpictures\n",
    "size = len(data_preprocessed.loc[data_preprocessed[\"hat_schäden\"] == False].index)\n",
    "if size < sample_size:\n",
    "    data_sample_temp.append(data_preprocessed.loc[data_preprocessed[\"hat_schäden\"] == False].sample(n=size, random_state=random_state).index)\n",
    "    print(\"Sample size for \\\"hat_schäden\\\" is \" + str(size) + \" instead of the wanted sample size of \" + str(sample_size) + \". Has not more entries then that.\")\n",
    "else:\n",
    "    data_sample_temp.append(data_preprocessed.loc[data_preprocessed[\"hat_schäden\"] == False].sample(n=sample_size, random_state=random_state).index)\n",
    "\n",
    "data_sample = pd.DataFrame(data_sample_temp)\n",
    "print(data_sample)\n",
    "testrow = 3\n",
    "valrow = 4\n",
    "data_test = pd.DataFrame(data_sample[testrow]) # nur in dem kleinen fall verwendbar wenn die daten noch nicht gelöscht wurden. \n",
    "data_val = pd.DataFrame(data_sample[valrow])\n",
    "data_train = data_sample.loc[:, data_sample.columns != testrow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verschiedene Datenlisten erstellen für die verwendung im coco dataset (filtern nach den korrekten daten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_list(data):\n",
    "    flat = data.to_numpy().flatten()\n",
    "    # list is optional, without it is a numpy array\n",
    "    index_list = list(flat[~np.isnan(flat)].astype(int))\n",
    "    return index_list\n",
    "\n",
    "\n",
    "datalist_train = into_list(data_train)\n",
    "datalist_val = into_list(data_val)\n",
    "# datalist_test = into_list(data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export data into seperate Datasets\n",
    "werden nicht weiter verwendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractsublist(originaldata, datalist):\n",
    "    dataset = originaldata.iloc[datalist]\n",
    "    return dataset\n",
    "\n",
    "def extract_nameending(data):\n",
    "    \"\"\"Input must have the column: image\"\"\"\n",
    "    data[\"image_name\"] = None\n",
    "    for i, row in data.iterrows():\n",
    "        data.loc[i, \"image_name\"] = row[\"image\"].split(\"/\")[-1]\n",
    "    data = data.drop(\"image\", axis=1)\n",
    "    return data\n",
    "\n",
    "def export(data, name):\n",
    "    filename = f\"{name}.json\"\n",
    "    data.to_json(filename)\n",
    "\n",
    "dataexport_train = extractsublist(data, datalist_train)\n",
    "dataexport_train = extract_nameending(dataexport_train)\n",
    "dataexport_train.head()\n",
    "# export(dataexport_train, \"Trainingdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Datasets\n",
    "https://cocodataset.org/#format-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export from COCO json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erstellen eines zweiten files aus dem die doppelten und nicht annotierten image daten gelöscht wurden.\n",
    "\n",
    "Diverese zeilen sind auskommentiert um zu verhindern, dass ausversehen Daten auf die festplatte geschrieben werden bei einem run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "path = \"../../../old/BAA/coco_annotations.json\"\n",
    "\n",
    "newpath = \"../../../old/BAA/Data/\"\n",
    "newfilename = \"coco_annotations_cleaned.json\"\n",
    "completeNewPath = \"../../../old/BAA/coco_annotations_cleaned.json\" # gecleantes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtern des hineingegebenen COCO files, anhand der YOLO Daten\n",
    "Nur nötig wenn das hineingeladene coco.json nicht zu verwendende daten enthält\n",
    "\n",
    "Nur einmal durchlaufen lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path, \"r\") as f:\n",
    "#     cocoOriginal = json.load(f)\n",
    "\n",
    "def createSubsetStructure(originalData, description):\n",
    "    \"\"\"erstellt eine COCO Struktur zum füllen und exportieren als json\"\"\"\n",
    "    subsetStructure = {\n",
    "        \"info\": [{\"year\": int(datetime.date.today().year)},\n",
    "                {\"version\": \"1.0\"},\n",
    "                {\"description\": description},\n",
    "                {\"contributer\": \"Michael Infanger\"},\n",
    "                {\"url\": \"\"},\n",
    "                {\"date_created\": str(datetime.datetime.now())}],\n",
    "        \"categories\": originalData[\"categories\"],\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "    return subsetStructure\n",
    "\n",
    "def clearCOCO(cocoOriginal, image_namen_ls):\n",
    "    imageIDlist = []\n",
    "    notUsedImageNames = []\n",
    "    descr = \"gecleantes COCO Datenset, ohne doppelte Bilder, ohne nicht annotierten Bilder, nur daten die in dem Ursprungsexport dieses Projekts enthalten waren\"\n",
    "    cocoNew = createSubsetStructure(cocoOriginal, description=descr)\n",
    "    minID = cocoOriginal[\"annotations\"][0][\"image_id\"] # kleineste ID der annotierten Daten\n",
    "    lastID = cocoOriginal[\"images\"][-1][\"id\"]\n",
    "    for n in range(minID, lastID + 1): #last ID included\n",
    "        cocoOriginal[\"images\"][n][\"file_name\"] = cocoOriginal[\"images\"][n][\"file_name\"].split(\"/\")[-1]\n",
    "        if cocoOriginal[\"images\"][n][\"file_name\"] in image_namen_ls:\n",
    "            imageIDlist.append(cocoOriginal[\"images\"][n][\"id\"])\n",
    "            cocoNew[\"images\"].append(cocoOriginal[\"images\"][n])\n",
    "        elif n == minID: # need this because the first three images begin with a %, this was used to format strings, and it is still working apparently. Because of it, this images werent catched with the first if statement...\n",
    "            imageIDlist.append(cocoOriginal[\"images\"][n][\"id\"])\n",
    "            cocoNew[\"images\"].append(cocoOriginal[\"images\"][n])\n",
    "        elif n == minID + 1:\n",
    "            imageIDlist.append(cocoOriginal[\"images\"][n][\"id\"])\n",
    "            cocoNew[\"images\"].append(cocoOriginal[\"images\"][n])\n",
    "        elif n == minID + 2:\n",
    "            imageIDlist.append(cocoOriginal[\"images\"][n][\"id\"])\n",
    "            cocoNew[\"images\"].append(cocoOriginal[\"images\"][n])\n",
    "        else: \n",
    "            notUsedImageNames.append(cocoOriginal[\"images\"][n][\"file_name\"])\n",
    "    for i in range(len(cocoOriginal[\"annotations\"])):\n",
    "        if cocoOriginal[\"annotations\"][i][\"image_id\"] in imageIDlist:\n",
    "            cocoNew[\"annotations\"].append(cocoOriginal[\"annotations\"][i])\n",
    "    return cocoNew\n",
    "\n",
    "def writeJson(jsonObject, filename):\n",
    "    if filename.split(\".\")[-1] != \"json\":\n",
    "        filename = f\"{filename}.json\"\n",
    "    with open(filename, \"w\") as g:\n",
    "        g.write(jsonObject)\n",
    "        \n",
    "# Uncomment if a new dataset needs to be loaded and cleared (thogether with the load statement)\n",
    "# cocoCleared = clearCOCO(cocoOriginal=cocoOriginal, image_namen_ls=image_namen_liste)\n",
    "# coco_cleared = json.dumps(cocoCleared, indent=4)\n",
    "# writeJson(coco_cleared, completeNewPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellen eines COCO Subsets (Train, Validation, Test)\n",
    "Testset ist ausgeklammert, da die Daten aus dem Ursprünglichen Datenset entfernt wurden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(completeNewPath, \"r\") as f:\n",
    "    cocox = json.load(f)\n",
    "\n",
    "# Say descriptions\n",
    "description_train = \"trainingset_small\"\n",
    "description_val = \"validierungsset_small\"\n",
    "description_test = \"testset_small\"\n",
    "\n",
    "# to use the original split from the YOLO format in the newer export of COCO that has high ID's\n",
    "def adapt_IDList(cocodata, subsetIDList):\n",
    "    namelist = []\n",
    "    ID_cleaned_list = []\n",
    "    for n in subsetIDList:\n",
    "        namelist.append(data_preprocessed[\"image_name\"].loc[n])\n",
    "    for i in cocodata[\"images\"]:\n",
    "        if i[\"file_name\"] in namelist:\n",
    "            ID_cleaned_list.append(i[\"id\"])\n",
    "    return ID_cleaned_list\n",
    "\n",
    "def createSubsetImages(subset, cocodata, subsetIDList):\n",
    "    # change file_name for the actual name without the whole path\n",
    "    for n in range(len(cocodata[\"images\"])):\n",
    "        # not needed with a cleaned dataset\n",
    "        # cocodata[\"images\"][n][\"file_name\"] = cocodata[\"images\"][n][\"file_name\"].split(\"/\")[-1]\n",
    "        if cocodata[\"images\"][n][\"id\"] in subsetIDList:\n",
    "            subset[\"images\"].append(cocodata[\"images\"][n])\n",
    "\n",
    "def createSubsetAnnotations(subset, cocodata, subsetIDList, label_filter=None):\n",
    "\tif label_filter == None:\n",
    "\t\tfor n in range(len(cocodata[\"annotations\"])):\n",
    "\t\t# not needed with a cleaned IDlist\n",
    "\t\t# cocodata[\"annotations\"][n][\"image_id\"] -= 20000\n",
    "\t\t\tif cocodata[\"annotations\"][n][\"image_id\"] in subsetIDList:\n",
    "\t\t\t\tsubset[\"annotations\"].append(cocodata[\"annotations\"][n])\n",
    "\telif type(label_filter) == int:\n",
    "\t\tfor n in range(len(cocodata[\"annotations\"])):\n",
    "\t\t# not needed with a cleaned IDlist\n",
    "\t\t# cocodata[\"annotations\"][n][\"image_id\"] -= 20000\n",
    "\t\t\tif cocodata[\"annotations\"][n][\"image_id\"] in subsetIDList:\n",
    "\t\t\t\tif cocodata[\"annotations\"][n][\"category_id\"] == label_filter:\n",
    "\t\t\t\t\tsubset[\"annotations\"].append(cocodata[\"annotations\"][n])\n",
    "\telse:\n",
    "\t\traise ValueError(\"Only None or Integer are accepted\")\n",
    "\n",
    "def createCOCOSubset(cocodata, subsetIDList, description, lb = None):\n",
    "    cocosub = createSubsetStructure(cocodata, description=description)\n",
    "    createSubsetImages(cocosub, cocodata, subsetIDList)\n",
    "    createSubsetAnnotations(cocosub, cocodata, subsetIDList, label_filter=lb)\n",
    "    return cocosub\n",
    "\n",
    "# clean subsetIDLists in order to be used with the coco dataset that uses way too hight id numbers\n",
    "d_train = adapt_IDList(cocox, datalist_train)\n",
    "d_val = adapt_IDList(cocox, datalist_val)\n",
    "# d_test = adapt_IDList(cocox, datalist_test)\n",
    "\n",
    "\n",
    "trainset = createCOCOSubset(cocox, d_train, description=description_train)\n",
    "valset = createCOCOSubset(cocox, d_val, description=description_val)\n",
    "# testset = createCOCOSubset(cocox, d_test, description=description_test)\n",
    "\n",
    "json_train = json.dumps(trainset, indent=4)\n",
    "json_val = json.dumps(valset, indent=4)\n",
    "# json_test = json.dumps(testset, indent=4)\n",
    "\n",
    "target_directory = \"../../../old/BAA/Annotierte_Bilder/2024-02-21-Schadenserkennung-Annotationen/\"\n",
    "\n",
    "\n",
    "# export data to separate files\n",
    "trainpath = newpath + \"train/\"\n",
    "valpath = newpath + \"val/\"\n",
    "# testpath = newpath + \"test/\"\n",
    "\n",
    "\n",
    "# writeJson(json_train, trainpath + \"coco_train.json\")\n",
    "# writeJson(json_val, valpath + \"coco_val.json\")\n",
    "# writeJson(json_test, testpath + \"coco_test.json\")\n",
    "\n",
    "\n",
    "image_folder = target_directory + \"images/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Löschen von Testdaten und verschieben/kopieren der restlichen Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyImages(image_list, target_dict):\n",
    "    images = glob.glob(os.path.join(image_folder, \"*.png\"))\n",
    "    for i in images:\n",
    "        basename = os.path.basename(i)\n",
    "        if basename in image_list:\n",
    "            shutil.copy(i, target_dict)\n",
    "\n",
    "def moveImages(image_list, target_dict):\n",
    "    images = glob.glob(os.path.join(image_folder, \"*.png\"))\n",
    "    for i in images:\n",
    "        basename = os.path.basename(i)\n",
    "        if basename in image_list:\n",
    "            shutil.move(i, target_dict)\n",
    "\n",
    "def delInfos(dataset, dataset_to_delete_stuff):\n",
    "    # should only be used once on the data for the testdata\n",
    "    # Iterate through dataset and delete any image and annotation that has the id of the image list\n",
    "    return_data = copy.deepcopy(dataset_to_delete_stuff)\n",
    "    img_id = []\n",
    "    run_idx = []\n",
    "    anno_idx = []\n",
    "    for n in range(len(dataset[\"images\"])):\n",
    "        for i in range(len(dataset_to_delete_stuff[\"images\"])):\n",
    "            if dataset[\"images\"][n][\"id\"] == dataset_to_delete_stuff[\"images\"][i][\"id\"]:\n",
    "                img_id.append(dataset_to_delete_stuff[\"images\"][i][\"id\"])\n",
    "                run_idx.append(i)\n",
    "        for y in range(len(dataset_to_delete_stuff[\"annotations\"])):\n",
    "            if dataset_to_delete_stuff[\"annotations\"][y][\"image_id\"] in img_id:\n",
    "                # klasse herausfiltern für dataset mit nur Wasser\n",
    "                anno_idx.append(y)\n",
    "    \n",
    "    delcount = 0\n",
    "\n",
    "    for z in run_idx:\n",
    "        del return_data[\"images\"][z-delcount]\n",
    "        delcount += 1\n",
    "    \n",
    "    delcount = 0\n",
    "\n",
    "    for x in anno_idx:\n",
    "        del return_data[\"annotations\"][x-delcount]\n",
    "        delcount += 1\n",
    "\n",
    "    return return_data\n",
    "\n",
    "\n",
    "def getImageNames(cocodata):\n",
    "    nameList = []\n",
    "    for n in cocodata[\"images\"]:\n",
    "        nameList.append(n[\"file_name\"])\n",
    "    return nameList\n",
    "\n",
    "train_names = getImageNames(trainset)\n",
    "val_names = getImageNames(valset)\n",
    "# test_names = getImageNames(testset)\n",
    "\n",
    "# copyImages(train_names, trainpath)\n",
    "# copyImages(val_names, valpath)\n",
    "# moveImages(test_names, testpath)\n",
    "\n",
    "# data_deleted = delInfos(testset, cocox)\n",
    "# d_delete = json.dumps(data_deleted, indent=4)\n",
    "# writeJson(d_delete, completeNewPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grosses Datenset erstellen (Mit allen Daten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexListg = []\n",
    "for i in range(len(cocox[\"images\"])):\n",
    "    indexListg.append(cocox[\"images\"][i][\"id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # für die reproduktion des Experiments\n",
    "splitvalue = int(len(indexListg) * 0.8) # 80/20 split von training und validationsdaten (Testdaten wurden in einem vorherigen Schritt bereits entfernt), bzw, wie viele sind 80% der Daten\n",
    "indexListn = np.asarray(indexListg, dtype=int)\n",
    "np.random.shuffle(indexListn)\n",
    "training, validation = indexListn[:splitvalue], indexListn[splitvalue:] # Datasplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainings und Validierungsset erstellen und in benannte ordner kopieren / verschieben, und Annotationen als json dazuschreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_train_max = \"trainingset_max\"\n",
    "description_val_max = \"validierungsset_max\"\n",
    "\n",
    "train_set_max = createCOCOSubset(cocox, training, description_train_max)\n",
    "val_set_max = createCOCOSubset(cocox, validation, description_val_max)\n",
    "\n",
    "json_train_max = json.dumps(train_set_max, indent=4)\n",
    "json_val_max = json.dumps(val_set_max, indent=4)\n",
    "\n",
    "save_path = \"../../../old/BAA/Data/\"\n",
    "train_set_max_path = os.path.join(save_path, \"train_max/coco_train_max.json\")\n",
    "val_set_max_path = os.path.join(save_path, \"val_max/coco_val_max.json\")\n",
    "\n",
    "#writeJson(json_train_max, train_set_max_path)\n",
    "#writeJson(json_val_max, val_set_max_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names_max = getImageNames(train_set_max)\n",
    "val_names_max = getImageNames(val_set_max)\n",
    "\n",
    "#copyImages(train_names_max, os.path.join(save_path, \"train_max/\"))\n",
    "#copyImages(val_names_max, os.path.join(save_path, \"val_max/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset mit nur Schmutzschäden nichts sonst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "splitvalue = int(len(indexListg) * 0.8)\n",
    "indexListn = np.asarray(indexListg, dtype=int)\n",
    "np.random.shuffle(indexListn)\n",
    "training, validation = indexListn[:splitvalue], indexListn[splitvalue:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_train = \"trainingset_Schmutz\"\n",
    "description_val = \"validierungsset_Schmutz\"\n",
    "\n",
    "train_set = createCOCOSubset(cocox, training, description_train, 3)\n",
    "val_set = createCOCOSubset(cocox, validation, description_val, 3)\n",
    "\n",
    "json_train = json.dumps(train_set, indent=4)\n",
    "json_val = json.dumps(val_set, indent=4)\n",
    "\n",
    "save_path = \"../../../old/BAA/Data/\"\n",
    "if os.path.exists(save_path) != True:\n",
    "    os.mkdir(save_path)\n",
    "folder_train = os.path.join(save_path, \"train_Schmutz/\")\n",
    "if os.path.exists(folder_train) != True:\n",
    "    os.mkdir(folder_train)\n",
    "folder_val = os.path.join(save_path, \"val_Schmutz/\")\n",
    "if os.path.exists(folder_val) != True:\n",
    "    os.mkdir(folder_val)\n",
    "train_set_path = os.path.join(folder_train, \"coco_train.json\")\n",
    "val_set_path = os.path.join(folder_val, \"coco_val.json\")\n",
    "\n",
    "#writeJson(json_train, train_set_path)\n",
    "#writeJson(json_val, val_set_path)\n",
    "\n",
    "train_names = getImageNames(train_set)\n",
    "val_names = getImageNames(val_set)\n",
    "\n",
    "#copyImages(train_names, folder_train)\n",
    "#copyImages(val_names, folder_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset mit nur Wasserschäden nichts sonst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "splitvalue = int(len(indexListg) * 0.8)\n",
    "indexListn = np.asarray(indexListg, dtype=int)\n",
    "np.random.shuffle(indexListn)\n",
    "training, validation = indexListn[:splitvalue], indexListn[splitvalue:]\n",
    "\n",
    "description_train = \"trainingset_Wasser\"\n",
    "description_val = \"validierungsset_Wasser\"\n",
    "\n",
    "train_set = createCOCOSubset(cocox, training, description_train, 9)\n",
    "val_set = createCOCOSubset(cocox, validation, description_val, 9)\n",
    "\n",
    "json_train = json.dumps(train_set, indent=4)\n",
    "json_val = json.dumps(val_set, indent=4)\n",
    "\n",
    "save_path = \"../../../old/BAA/Data/\"\n",
    "if os.path.exists(save_path) != True:\n",
    "    os.mkdir(save_path)\n",
    "folder_train = os.path.join(save_path, \"train_Wasser/\")\n",
    "if os.path.exists(folder_train) != True:\n",
    "    os.mkdir(folder_train)\n",
    "folder_val = os.path.join(save_path, \"val_Wasser/\")\n",
    "if os.path.exists(folder_val) != True:\n",
    "    os.mkdir(folder_val)\n",
    "train_set_path = os.path.join(folder_train, \"coco_train.json\")\n",
    "val_set_path = os.path.join(folder_val, \"coco_val.json\")\n",
    "\n",
    "#writeJson(json_train, train_set_path)\n",
    "#writeJson(json_val, val_set_path)\n",
    "\n",
    "train_names = getImageNames(train_set)\n",
    "val_names = getImageNames(val_set)\n",
    "\n",
    "#copyImages(train_names, folder_train)\n",
    "#copyImages(val_names, folder_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset mit nur Schäden nichts sonst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "splitvalue = int(len(indexListg) * 0.8)\n",
    "indexListn = np.asarray(indexListg, dtype=int)\n",
    "np.random.shuffle(indexListn)\n",
    "training, validation = indexListn[:splitvalue], indexListn[splitvalue:]\n",
    "\n",
    "description_train = \"trainingset_Schäden\"\n",
    "description_val = \"validierungsset_Schäden\"\n",
    "\n",
    "train_set = createCOCOSubset(cocox, training, description_train, 4)\n",
    "val_set = createCOCOSubset(cocox, validation, description_val, 4)\n",
    "\n",
    "json_train = json.dumps(train_set, indent=4)\n",
    "json_val = json.dumps(val_set, indent=4)\n",
    "\n",
    "save_path = \"../../../old/BAA/Data/\"\n",
    "if os.path.exists(save_path) != True:\n",
    "    os.mkdir(save_path)\n",
    "folder_train = os.path.join(save_path, \"train_Schäden/\")\n",
    "if os.path.exists(folder_train) != True:\n",
    "    os.mkdir(folder_train)\n",
    "folder_val = os.path.join(save_path, \"val_Schäden/\")\n",
    "if os.path.exists(folder_val) != True:\n",
    "    os.mkdir(folder_val)\n",
    "train_set_path = os.path.join(folder_train, \"coco_train.json\")\n",
    "val_set_path = os.path.join(folder_val, \"coco_val.json\")\n",
    "\n",
    "#writeJson(json_train, train_set_path)\n",
    "#writeJson(json_val, val_set_path)\n",
    "\n",
    "train_names = getImageNames(train_set)\n",
    "val_names = getImageNames(val_set)\n",
    "\n",
    "#copyImages(train_names, folder_train)\n",
    "#copyImages(val_names, folder_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset mit nur Sonstigen Sachen nichts sonst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "splitvalue = int(len(indexListg) * 0.8)\n",
    "indexListn = np.asarray(indexListg, dtype=int)\n",
    "np.random.shuffle(indexListn)\n",
    "training, validation = indexListn[:splitvalue], indexListn[splitvalue:]\n",
    "\n",
    "description_train = \"trainingset_Sonstiges\"\n",
    "description_val = \"validierungsset_Sonstiges\"\n",
    "\n",
    "train_set = createCOCOSubset(cocox, training, description_train, 6)\n",
    "val_set = createCOCOSubset(cocox, validation, description_val, 6)\n",
    "\n",
    "json_train = json.dumps(train_set, indent=4)\n",
    "json_val = json.dumps(val_set, indent=4)\n",
    "\n",
    "save_path = \"../../../old/BAA/Data/\"\n",
    "if os.path.exists(save_path) != True:\n",
    "    os.mkdir(save_path)\n",
    "folder_train = os.path.join(save_path, \"train_Sonstiges/\")\n",
    "if os.path.exists(folder_train) != True:\n",
    "    os.mkdir(folder_train)\n",
    "folder_val = os.path.join(save_path, \"val_Sonstiges/\")\n",
    "if os.path.exists(folder_val) != True:\n",
    "    os.mkdir(folder_val)\n",
    "train_set_path = os.path.join(folder_train, \"coco_train.json\")\n",
    "val_set_path = os.path.join(folder_val, \"coco_val.json\")\n",
    "\n",
    "#writeJson(json_train, train_set_path)\n",
    "#writeJson(json_val, val_set_path)\n",
    "\n",
    "train_names = getImageNames(train_set)\n",
    "val_names = getImageNames(val_set)\n",
    "\n",
    "#copyImages(train_names, folder_train)\n",
    "#copyImages(val_names, folder_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset mit nur Schimmelschäden nichts sonst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "splitvalue = int(len(indexListg) * 0.8)\n",
    "indexListn = np.asarray(indexListg, dtype=int)\n",
    "np.random.shuffle(indexListn)\n",
    "training, validation = indexListn[:splitvalue], indexListn[splitvalue:]\n",
    "\n",
    "description_train = \"trainingset_Schimmel\"\n",
    "description_val = \"validierungsset_Schimmel\"\n",
    "\n",
    "train_set = createCOCOSubset(cocox, training, description_train, 2)\n",
    "val_set = createCOCOSubset(cocox, validation, description_val, 2)\n",
    "\n",
    "json_train = json.dumps(train_set, indent=4)\n",
    "json_val = json.dumps(val_set, indent=4)\n",
    "\n",
    "save_path = \"../../../old/BAA/Data/\"\n",
    "if os.path.exists(save_path) != True:\n",
    "    os.mkdir(save_path)\n",
    "folder_train = os.path.join(save_path, \"train_Schimmel/\")\n",
    "if os.path.exists(folder_train) != True:\n",
    "    os.mkdir(folder_train)\n",
    "folder_val = os.path.join(save_path, \"val_Schimmel/\")\n",
    "if os.path.exists(folder_val) != True:\n",
    "    os.mkdir(folder_val)\n",
    "train_set_path = os.path.join(folder_train, \"coco_train.json\")\n",
    "val_set_path = os.path.join(folder_val, \"coco_val.json\")\n",
    "\n",
    "#writeJson(json_train, train_set_path)\n",
    "#writeJson(json_val, val_set_path)\n",
    "\n",
    "train_names = getImageNames(train_set)\n",
    "val_names = getImageNames(val_set)\n",
    "\n",
    "#copyImages(train_names, folder_train)\n",
    "#copyImages(val_names, folder_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset mit nur Schädlingen nichts sonst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "splitvalue = int(len(indexListg) * 0.8)\n",
    "indexListn = np.asarray(indexListg, dtype=int)\n",
    "np.random.shuffle(indexListn)\n",
    "training, validation = indexListn[:splitvalue], indexListn[splitvalue:]\n",
    "\n",
    "description_train = \"trainingset_Schädlinge\"\n",
    "description_val = \"validierungsset_Schädlinge\"\n",
    "\n",
    "train_set = createCOCOSubset(cocox, training, description_train, 5)\n",
    "val_set = createCOCOSubset(cocox, validation, description_val, 5)\n",
    "\n",
    "json_train = json.dumps(train_set, indent=4)\n",
    "json_val = json.dumps(val_set, indent=4)\n",
    "\n",
    "save_path = \"../../../old/BAA/Data/\"\n",
    "if os.path.exists(save_path) != True:\n",
    "    os.mkdir(save_path)\n",
    "folder_train = os.path.join(save_path, \"train_Schädlinge/\")\n",
    "if os.path.exists(folder_train) != True:\n",
    "    os.mkdir(folder_train)\n",
    "folder_val = os.path.join(save_path, \"val_Schädlinge/\")\n",
    "if os.path.exists(folder_val) != True:\n",
    "    os.mkdir(folder_val)\n",
    "train_set_path = os.path.join(folder_train, \"coco_train.json\")\n",
    "val_set_path = os.path.join(folder_val, \"coco_val.json\")\n",
    "\n",
    "#writeJson(json_train, train_set_path)\n",
    "#writeJson(json_val, val_set_path)\n",
    "\n",
    "train_names = getImageNames(train_set)\n",
    "val_names = getImageNames(val_set)\n",
    "\n",
    "#copyImages(train_names, folder_train)\n",
    "#copyImages(val_names, folder_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeile 216 hat die meisten schadenseinträge, gesamt 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "\n",
    "1. Es ist im grossen und ganzen ein guter Datensatz\n",
    "    - musste keine rows löschen\n",
    "2. Die verlinkung der einzelnen Bilder sind vollständig\n",
    "3. Die Datenmenge ist sehr dürftig für einen Ai algorithmus\n",
    "    - Data Augmentation als gegenmassnahme\n",
    "4. Die verteilung der Daten ist sehr ungleich. Bsp. es hat 871 Bilder mit Schmutzschäden aber nur vier mit Schädlingen\n",
    "    - Es braucht mehr Daten, es ist eine überlegung wert die fälle mit den wenigsten vorkommnissen nicht zu gebrauchen oder erst in einem späteren stadium, alternativ könnten im allgemeinen wenige daten verwendet werden um die unterschiede in den mengen zu verkleinern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
