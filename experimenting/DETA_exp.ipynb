{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETA\n",
    "\n",
    "## Version 1.0\n",
    "\n",
    "Experiment mit standard DETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, torchvision, torchaudio\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "import json\n",
    "import copy\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets, tv_tensors\n",
    "from torchvision.io import read_image\n",
    "from coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from transformers import AutoImageProcessor, DetaForObjectDetection\n",
    "from transformers import DetaConfig, DetaImageProcessor\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import Image, ImageDraw\n",
    "from mean_average_precision import MetricBuilder\n",
    "\n",
    "HyperparameterSweep = False\n",
    "Training = False\n",
    "\n",
    "if HyperparameterSweep and Training:\n",
    "    raise ValueError(\"Its not advised to use both at the same time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, processor, image_name, transforms=None):\n",
    "        ann_file = os.path.join(img_folder, image_name)\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file, transforms)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        # feel free to add data augmentation here before passing them to the next step\n",
    "        \n",
    "\t\t\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target\n",
    "\n",
    "processor = DetaImageProcessor.from_pretrained(\"jozhang97/deta-resnet-50\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create Train and Validation Dataset\n",
    "\n",
    "    #Minimal Dataset\n",
    "# train_dataset = CocoDetection(img_folder='../../../old/BAA/Data/train', processor=processor, image_name=\"coco_train.json\")\n",
    "# val_dataset = CocoDetection(img_folder='../../../old/BAA/Data/val', processor=processor, image_name=\"coco_val.json\")\n",
    "\n",
    "    #Maximal Dataset\n",
    "#train_dataset = CocoDetection(img_folder='../../../old/BAA/Data/train_max', processor=processor, image_name=\"coco_train_max.json\")\n",
    "#val_dataset = CocoDetection(img_folder='../../../old/BAA/Data/val_max', processor=processor, image_name=\"coco_val_max.json\")\n",
    "\n",
    "    # Dataset with Augmentations\n",
    "train_dataset = CocoDetection(img_folder='../../../old/BAA/Data/train_combined', processor=processor, image_name=\"coco_Combined.json\")\n",
    "val_dataset = CocoDetection(img_folder='../../../old/BAA/Data/val_combined', processor=processor, image_name=\"coco_Combined.json\")\n",
    "\n",
    "test_dataset = CocoDetection(img_folder='../../../old/BAA/Data/test', processor=processor, image_name=\"coco_test.json\")\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))\n",
    "\n",
    "\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "def collate_fn(batch):\n",
    "\tpixel_values = [item[0] for item in batch]\n",
    "\tencoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "\tlabels = [item[1] for item in batch]\n",
    "\tbatch = {}\n",
    "\tbatch['pixel_values'] = encoding['pixel_values']\n",
    "\tbatch['pixel_mask'] = encoding['pixel_mask']\n",
    "\tbatch['labels'] = labels\n",
    "\treturn batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define DETA\n",
    "\n",
    "outcommented lines reflect the first try with torchmetrics, the goal was to get the AP on runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deta(pl.LightningModule):\n",
    "    def __init__(self, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "        # replace COCO classification head with custom head\n",
    "        # we specify the \"no_timm\" variant here to not rely on the timm library\n",
    "        # for the convolutional backbone\n",
    "        self.model = DetaForObjectDetection.from_pretrained(\"jozhang97/deta-resnet-50\",\n",
    "                                                            num_labels=len(id2label),\n",
    "                                                            auxiliary_loss=True,\n",
    "                                                            ignore_mismatched_sizes=True)\n",
    "        # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "        #self.processor = DetaImageProcessor.from_pretrained(\"jozhang97/deta-resnet-50\")\n",
    "        self.save_hyperparameters()\n",
    "        #self.val_epoch_count = 0 # to not log at the first run\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "        #self.val_ap = []\n",
    "        #self.train_ap = []\n",
    "        #self.train_metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=9)\n",
    "        #self.val_metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=9)\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx, t_v):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        \n",
    "\n",
    "        # turn into a list of dictionaries (one item for each example in the batch)\n",
    "        #orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "        #results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0.02)\n",
    "        \n",
    "        #gt = []\n",
    "        #size = batch[\"labels\"][0][\"class_labels\"].size()\n",
    "        #for i in range(0, size[0]):\n",
    "        #    n = []\n",
    "        #    n.extend(batch[\"labels\"][0][\"boxes\"][i])\n",
    "        #    n.extend([batch[\"labels\"][0][\"class_labels\"][i]])\n",
    "        #    n.extend([0]) # difficulty\n",
    "        #    n.extend([batch[\"labels\"][0][\"iscrowd\"][i]])\n",
    "        #    gt.append(n)\n",
    "\n",
    "        #preds = []\n",
    "        #sizep = len(results[0][\"labels\"])\n",
    "        #for i in range(0, sizep):\n",
    "        #    m = []\n",
    "        #    m.extend(results[0][\"boxes\"][i])\n",
    "        #    m.extend([results[0][\"labels\"][i]])\n",
    "        #    m.extend([results[0][\"scores\"][i]])\n",
    "        #    preds.append(m)\n",
    "\n",
    "        #gt = torch.tensor(gt)\n",
    "        #preds = torch.tensor(preds)\n",
    "        #if t_v:\n",
    "        #\tself.data_prep_train(pixel_values, pixel_mask, labels)\n",
    "        #else:\n",
    "        #\tself.data_prep_val(pixel_values, pixel_mask, labels)\n",
    "\n",
    "        return loss, loss_dict #, preds, gt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # add: , preds, gt\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx, True)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        #self.train_metric_fn.add(preds, gt)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "        return loss\n",
    "\n",
    "    \"\"\"def on_train_epoch_end(self) -> None:\n",
    "        metrics = self.train_metric_fn.value(iou_thresholds=np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy='soft')\n",
    "        ap = metrics[\"mAP\"]\n",
    "        print(ap)\n",
    "        self.train_ap.append(ap)\n",
    "        #wandb.log({\"train_epoch_AP\": ap})\n",
    "        self.train_metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=9)\"\"\"\n",
    "\n",
    "    \"\"\"def on_train_epoch_end(self) -> None:\n",
    "        print(\"onTrainEpoch\")\n",
    "        t1 = time.time()\n",
    "        self.cmetric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "        self.cmetric.update(self.training_step_pred, self.training_step_targ)\n",
    "        result = self.cmetric.compute()\n",
    "        counter = 0\n",
    "        for k, v in result.items():\n",
    "            counter += 1\n",
    "            if counter < 15:\n",
    "                wandb.log({\"train_epoch_\" + k: v.item()})\n",
    "                print({\"train_\" + k: v.item()})\n",
    "        self.training_step_pred = []\n",
    "        self.training_step_targ = []\n",
    "        t2 = time.time()\n",
    "        print(round(t2 - t1, 4))\"\"\"\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # add: , preds, gt\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx, False)\n",
    "        #self.val_metric_fn.add(preds, gt)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "        return loss\n",
    "\n",
    "    \"\"\"def on_validation_epoch_end(self) -> None:\n",
    "        vmetrics = self.val_metric_fn.value(iou_thresholds=np.arange(0.5, 1.0, 0.05), recall_thresholds=np.arange(0., 1.01, 0.01), mpolicy='soft')\n",
    "        ap = vmetrics[\"mAP\"]\n",
    "        print(ap)\n",
    "        self.val_ap.append(ap)\n",
    "        wandb.log({\"val_epoch_AP\": ap})\n",
    "        self.val_metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=9)\"\"\"\n",
    "\n",
    "    \"\"\"def on_validation_epoch_end(self) -> None:\n",
    "        if self.val_epoch_count == 1:\n",
    "            print(\"onValEpoch\")\n",
    "            t1 = time.time()\n",
    "            self.cmetric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "            self.cmetric.update(self.training_step_pred, self.training_step_targ)\n",
    "            result = self.cmetric.compute()\n",
    "            counter = 0\n",
    "            for k, v in result.items():\n",
    "                counter += 1\n",
    "                if counter < 15:\n",
    "                    wandb.log({\"validation_epoch_\" + k: v.item()})\n",
    "                    print({\"validation_epoch_\" + k: v.item()})\n",
    "            self.validation_step_pred = []\n",
    "            self.validation_step_targ = []\n",
    "            t2 = time.time()\n",
    "            print(round(t2 - t1, 4))\n",
    "        else:\n",
    "            self.val_epoch_count = 1\"\"\"\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "                {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                    \"lr\": self.lr_backbone,\n",
    "                },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                    weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    \"\"\"def data_prep_train(self, pixel_values, pixel_mask, labels):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "        results = self.processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n",
    "        for n in results:\n",
    "            self.training_step_pred.append(n)\n",
    "        for i in labels:\n",
    "            self.training_step_targ.append({\"boxes\":i[\"boxes\"], \"labels\":i[\"class_labels\"]})\n",
    "\n",
    "    def data_prep_val(self, pixel_values, pixel_mask, labels):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "        results = self.processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n",
    "        for n in results:\n",
    "            self.validation_step_pred.append(n)\n",
    "        for i in labels:\n",
    "            self.validation_step_targ.append({\"boxes\":i[\"boxes\"], \"labels\":i[\"class_labels\"]})\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define Weights & Biases Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HyperparameterSweep:\n",
    "    import pprint\n",
    "    sweep_config = {\n",
    "        \"name\": \"Sweeps_DETA\",\n",
    "        \"method\":\"random\"\n",
    "    }\n",
    "    metric = {\n",
    "        \"name\": \"validation_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    }\n",
    "\n",
    "    sweep_config[\"metric\"] = metric\n",
    "\n",
    "    parameters_dict = {\n",
    "        \"learning_rate_backbone\" : {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.000001,\n",
    "            \"max\": 0.001\n",
    "        },\n",
    "        \"learning_rate_transformer\" : {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.000001,\n",
    "            \"max\": 0.001\n",
    "        },\n",
    "        \"weight_decay\" : {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.000001,\n",
    "            \"max\": 0.001\n",
    "        },\n",
    "    }\n",
    "    sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "    pprint.pprint(sweep_config)\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"BAA_Book_Damage_Detection_Sweeps\")\n",
    "    sweep_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "if HyperparameterSweep:\n",
    "    def train(config=None):\n",
    "        # login to weights and biases, to relogin: wandb.login(key=\"YOUR KEY\", relogin=True)\n",
    "        wandb.login()\n",
    "\n",
    "        with wandb.init(config=config):\n",
    "\n",
    "            config = wandb.config\n",
    "\n",
    "            #date = datetime.datetime.now()\n",
    "            #run_name = \"Run at the {}\".format(date)\n",
    "\n",
    "            lr = config.learning_rate_transformer #1e-4 original\n",
    "            lr_backbone=config.learning_rate_backbone #1e-5 original\n",
    "            weight_decay=config.weight_decay #1e-4 original\n",
    "\n",
    "            project = \"BAA_Book_Damage_Detection_Sweeps\"\n",
    "\n",
    "            model = Deta(lr=lr, lr_backbone=lr_backbone, weight_decay=weight_decay)\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            # outputs = model(pixel_values=batch['pixel_values'].to(device), pixel_mask=batch['pixel_mask'].to(device))\n",
    "            # print(outputs.logits.shape)\n",
    "\n",
    "            max_steps_var = 5500\n",
    "\n",
    "            wandb_logger = WandbLogger(\"DETA\", \"../models/\", project=project, log_model=True, checkpoint_name=f\"DETA_Sweep_{max_steps_var}_Steps\")\n",
    "            trainer = Trainer(max_steps=max_steps_var, gradient_clip_val=0.1, logger=wandb_logger)\n",
    "            trainer.fit(model)\n",
    "            \n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Training:\n",
    "    # login to weights and biases, to relogin: wandb.login(key=\"YOUR KEY\", relogin=True)\n",
    "    wandb.login()\n",
    "\n",
    "    date = datetime.datetime.now()\n",
    "    run_name = \"Run at the {}\".format(date)\n",
    "\n",
    "    lr =2.12e-4 #1e-4\n",
    "    lr_backbone=4.87e-5 #1e-5\n",
    "    weight_decay=3.33e-5 #1e-4\n",
    "\n",
    "    project = \"BAA_Book_Damage_Detection\"\n",
    "\n",
    "    wandb.init(\n",
    "        project=project,\n",
    "        name=run_name,\n",
    "        config={\"learning_rate_transformer\": lr,\n",
    "                \"learning_rate_backbone\": lr_backbone,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"architecture\": \"DETA\",\n",
    "                \"dataset\": \"test_normal_set\",\n",
    "                \"train_set_size\": len(train_dataset)}\n",
    "        )\n",
    "\n",
    "    model = Deta(lr=lr, lr_backbone=lr_backbone, weight_decay=weight_decay)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    #outputs = model(pixel_values=batch['pixel_values'].to(device), pixel_mask=batch['pixel_mask'].to(device))\n",
    "    #print(outputs.logits.shape)\n",
    "\n",
    "    max_steps_var = 34368\n",
    "\n",
    "    wandb_logger = WandbLogger(\"Optimize_Logging\", \"../models/\", project=project, log_model=True, checkpoint_name=f\"DETA_{max_steps_var}_Steps\")\n",
    "    trainer = Trainer(max_steps=max_steps_var, gradient_clip_val=0.1, logger=wandb_logger)\n",
    "    trainer.fit(model)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HyperparameterSweep:\n",
    "    wandb.agent(sweep_id=sweep_id, function=train)\n",
    "    \n",
    "    wandb.finish()\n",
    "    wandb.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the checkpoint\n",
    "\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('damaged-books-detection-ml/BAA_Book_Damage_Detection/DETA_300_Steps:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "model = Deta.load_from_checkpoint(artifact_dir + \"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the checkpoint\n",
    "\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('damaged-books-detection-ml/BAA_Book_Damage_Detection_Sweeps/DETA_Sweep_6000_Steps:v7', type='model')\n",
    "# artifact = run.use_artifact(\"../models/ut10b6wq/epoch=108-step=93631.ckpt\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "model = Deta.load_from_checkpoint(artifact_dir + \"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "def prepare_for_coco_detection(predictions):\n",
    "    coco_results = []\n",
    "    for original_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = convert_to_xywh(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        coco_results.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": labels[k],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": scores[k],\n",
    "                }\n",
    "                for k, box in enumerate(boxes)\n",
    "            ]\n",
    "        )\n",
    "    return coco_results\n",
    "\n",
    "# initialize evaluator with ground truth (gt)\n",
    "evaluator = CocoEvaluator(coco_gt=test_dataset.coco, iou_types=[\"bbox\"])\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    # turn into a list of dictionaries (one item for each example in the batch)\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0.0\n",
    "                                                      )\n",
    "\n",
    "    # provide to metric\n",
    "    # metric expects a list of dictionaries, each item\n",
    "    # containing image_id, category_id, bbox and score keys\n",
    "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    predictions = prepare_for_coco_detection(predictions)\n",
    "    evaluator.update(predictions)\n",
    "\n",
    "evaluator.synchronize_between_processes()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "# TODO for a specific label use one specific color\n",
    "for z in range(6):\n",
    "    pixel_values, target = test_dataset[z]\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    annotations = test_dataset.coco.imgToAnns[\"image_id\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward pass to get class logits and bounding boxes\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "\n",
    "    # colors for visualization\n",
    "    COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "            [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "    def plot_results(pil_img, scores, labels, boxes, anno):\n",
    "        plt.figure(figsize=(16,10))\n",
    "        plt.imshow(pil_img)\n",
    "        ax = plt.gca()\n",
    "        colors = COLORS * 100\n",
    "        types = {}\n",
    "        for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                fill=False, color=c, linewidth=3))\n",
    "            text = f'{id2label[label]}: {score:0.2f}'\n",
    "            textx = text.split(\": \")\n",
    "            if textx[0] in types:\n",
    "                types.update({textx[0]: types[textx[0]] + 1})\n",
    "            else: \n",
    "                types.update({textx[0]:1})\n",
    "            ax.text(xmin, ymin, text, fontsize=10,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "        print(types)\n",
    "        \n",
    "        # draw annotatet boxes\n",
    "        typesAnno = {}\n",
    "        color = [0.000, 0.000, 0.000]\n",
    "        # TODO Resize bbox\n",
    "        for annot in anno:\n",
    "            (x, y, w, h) = annot[\"bbox\"]\n",
    "            (xmin, ymin, xmax, ymax) = (x, y, x + w, y + h)\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                fill=False, color=color, linewidth=3))\n",
    "            text = f'{id2label[annot[\"category_id\"]]}'\n",
    "            if text in typesAnno:\n",
    "                typesAnno.update({text: typesAnno[text] + 1})\n",
    "            else: \n",
    "                typesAnno.update({text:1})\n",
    "            ax.text(xmin, ymin, text, fontsize=0,\n",
    "                    bbox=dict(facecolor='black', alpha=0.0))\n",
    "        print(typesAnno)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # load image based on ID\n",
    "    image_id = target['image_id'].item()\n",
    "    image = test_dataset.coco.loadImgs(image_id)[0]\n",
    "    anno = test_dataset.coco.imgToAnns[image_id]\n",
    "    image = Image.open(os.path.join('../../../old/BAA/Data/test', image['file_name']))\n",
    "\n",
    "    # postprocess model outputs\n",
    "    width, height = image.size\n",
    "    postprocessed_outputs = processor.post_process_object_detection(outputs,\n",
    "                                                                    target_sizes=[(height, width)],\n",
    "                                                                    threshold=0.18)\n",
    "    results = postprocessed_outputs[0]\n",
    "    print(results)\n",
    "    plot_results(image, results['scores'], results['labels'], results['boxes'], anno)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing AP computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'labels': [{'size': torch.tensor([ 800, 1296], device='cuda:0'), 'image_id': torch.tensor([20443], device='cuda:0'), 'class_labels': torch.tensor([3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3], device='cuda:0'), 'boxes': torch.tensor([[0.2096, 0.4859, 0.0399, 0.0215],\n",
    "        [0.2438, 0.4840, 0.0172, 0.0364],\n",
    "        [0.2883, 0.3713, 0.0383, 0.2544],\n",
    "        [0.3304, 0.4504, 0.0321, 0.1073],\n",
    "        [0.4138, 0.5002, 0.0445, 0.0279],\n",
    "        [0.3943, 0.4495, 0.0798, 0.1226],\n",
    "        [0.6195, 0.4441, 0.0096, 0.0776],\n",
    "        [0.6457, 0.4289, 0.0181, 0.1400],\n",
    "        [0.6899, 0.2837, 0.0076, 0.0479],\n",
    "        [0.6951, 0.3741, 0.0092, 0.1248],\n",
    "        [0.1875, 0.7129, 0.0057, 0.0093],\n",
    "        [0.5074, 0.6600, 0.0050, 0.0074],\n",
    "        [0.6039, 0.7395, 0.0078, 0.0371],\n",
    "        [0.6086, 0.6238, 0.0057, 0.0160],\n",
    "        [0.7040, 0.6147, 0.0073, 0.1277],\n",
    "        [0.5725, 0.4416, 0.0511, 0.1027],\n",
    "        [0.2317, 0.6155, 0.0192, 0.0892]], device='cuda:0'), 'area': torch.tensor([ 847.6823,  419.4721, 9632.8975, 2344.4541,  324.3682, 6766.8066,\n",
    "         560.3406, 2288.1863,  183.3673,  661.5347,   36.9824,   29.0828,\n",
    "         236.8991,   67.3009,  397.1856, 4885.4731, 1037.4435],\n",
    "       device='cuda:0'), 'iscrowd': torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), 'orig_size': torch.tensor([1200, 1944], device='cuda:0')}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"labels\"][0][\"class_labels\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, 0):\n",
    "    print(\"djalkfj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([0.2438, 0.4840, 0.0172, 0.0364])\n",
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if z == torch.tensor([]):\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [{'scores': torch.tensor([0.6273, 0.6238, 0.6178, 0.6165, 0.6139, 0.6118, 0.6084, 0.6084, 0.6073,\n",
    "        0.6072, 0.6066, 0.6059, 0.6058, 0.6054, 0.6054, 0.6052, 0.6050, 0.6043,\n",
    "        0.6025, 0.6020, 0.6017, 0.6010, 0.6002, 0.5995, 0.5993, 0.5990, 0.5989,\n",
    "        0.5988, 0.5964, 0.5962, 0.5960, 0.5960, 0.5941, 0.5933, 0.5930, 0.5920,\n",
    "        0.5916, 0.5912, 0.5880, 0.5872, 0.5858, 0.5846, 0.5838, 0.5837, 0.5831,\n",
    "        0.5831, 0.5828, 0.5819, 0.5809, 0.5807, 0.5802, 0.5801, 0.5799, 0.5775,\n",
    "        0.5769, 0.5766, 0.5758, 0.5749, 0.5746, 0.5742, 0.5740, 0.5734, 0.5717,\n",
    "        0.5704, 0.5696, 0.5695, 0.5689, 0.5686, 0.5684, 0.5684, 0.5683, 0.5669,\n",
    "        0.5665, 0.5662, 0.5658, 0.5651, 0.5646, 0.5643, 0.5640, 0.5640, 0.5635,\n",
    "        0.5632, 0.5631, 0.5629, 0.5628, 0.5628, 0.5623, 0.5614, 0.5613, 0.5612,\n",
    "        0.5612, 0.5609, 0.5606, 0.5605, 0.5604, 0.5600, 0.5600, 0.5594, 0.5592,\n",
    "        0.5590], device='cuda:0'), 'labels': torch.tensor([2, 2, 4, 9, 4, 2, 2, 2, 4, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
    "        4, 2, 2, 9, 9, 4, 4, 9, 4, 9, 9, 4, 9, 4, 4, 2, 9, 4, 9, 4, 9, 9, 2, 4,\n",
    "        2, 2, 2, 2, 9, 2, 9, 9, 9, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 9, 9, 2, 9,\n",
    "        9, 4, 0, 9, 4, 4, 9, 4, 4, 4, 4, 4, 0, 4, 9, 4, 4, 4, 4, 9, 4, 4, 4, 4,\n",
    "        4, 4, 0, 4], device='cuda:0'), 'boxes': torch.tensor([[ 676.2388,  277.1254,  850.7404,  625.0176],\n",
    "        [ 491.5782,  608.8383,  664.5103,  935.7572],\n",
    "        [ 245.1959,   30.2981, 1908.2277, 1173.9956],\n",
    "        [ 862.0541, 1072.7997,  989.9567, 1112.1863],\n",
    "        [ 317.2450,   22.0593, 1450.0930,  330.5922],\n",
    "        [ 388.6223,    5.3880,  637.9678,  199.8394],\n",
    "        [ 606.6195,  338.1898,  677.1177,  614.6801],\n",
    "        [ 832.0270,  614.6859,  973.1371,  932.7524],\n",
    "        [ 288.2072,   10.3697, 1837.5369,  447.6088],\n",
    "        [ 764.4576,  160.3179,  911.5189,  198.4788],\n",
    "        [ 701.6495,  159.8817,  943.4083,  194.7039],\n",
    "        [ 793.9598,  158.5940,  946.8500,  197.9476],\n",
    "        [ 271.4940,   72.6817, 1910.3503,  611.1729],\n",
    "        [ 970.0051,  106.4567, 1097.7736,  142.6047],\n",
    "        [ 843.1724,  106.4497,  961.6071,  141.5147],\n",
    "        [ 984.1047,  154.2677, 1203.7831,  187.1603],\n",
    "        [ 889.5905,  106.8443, 1006.0449,  140.3231],\n",
    "        [ 748.9282,  158.6208,  997.1929,  191.0836],\n",
    "        [ 898.5062,  106.3222, 1101.9955,  144.9065],\n",
    "        [ 761.7839,  156.8068, 1050.6472,  187.2452],\n",
    "        [ 856.7856,  161.5986, 1078.5540,  198.9951],\n",
    "        [ 629.3372,   72.2920,  706.5010,  130.5994],\n",
    "        [ 831.1205,  105.8166,  992.0811,  136.3994],\n",
    "        [ 735.0187,  104.6248,  998.1118,  135.4265],\n",
    "        [ 362.8256,    7.4454, 1430.8937,  251.0522],\n",
    "        [1062.8165,  287.1315, 1233.4741,  603.4262],\n",
    "        [ 407.9402,  606.9929,  644.2662,  940.0078],\n",
    "        [ 783.8613,  153.9332, 1066.2512,  181.5553],\n",
    "        [ 823.7746,  105.5136, 1064.1918,  135.8379],\n",
    "        [ 204.1723,  598.1271, 1878.0404, 1177.5947],\n",
    "        [ 288.9090,  200.1757, 1437.0602,  973.5871],\n",
    "        [ 855.7324,  163.4534, 1164.6575,  198.6117],\n",
    "        [ 270.7104,   91.4500, 1456.1766,  700.7239],\n",
    "        [ 920.4334,  106.2333, 1166.8995,  135.2824],\n",
    "        [ 879.5668,  105.8654, 1106.7937,  135.1730],\n",
    "        [ 233.2137,  387.9187, 1509.7170, 1175.8739],\n",
    "        [ 710.6248,   79.1035,  947.1807,  110.2706],\n",
    "        [ 287.1901,  245.8605, 1421.9178,  622.1282],\n",
    "        [ 744.9787,   46.2645, 1857.5127, 1153.2631],\n",
    "        [ 444.7609,  288.1731,  637.1275,  608.7245],\n",
    "        [ 841.8706,  217.1818,  986.1880,  246.9627],\n",
    "        [ 264.0420,  216.1142, 1428.3958,  332.6553],\n",
    "        [ 796.8151,  217.5832,  891.8583,  247.7887],\n",
    "        [ 398.1796,   59.0355, 1419.2971,  211.7155],\n",
    "        [1066.6624,  211.2775, 1304.0228,  244.4843],\n",
    "        [1087.7819,  201.9277, 1307.7440,  235.8141],\n",
    "        [ 612.2525,  621.8962,  757.0402,  935.7299],\n",
    "        [ -19.4005,  976.6158, 1795.9170, 1188.5854],\n",
    "        [ 400.7997,    4.5456,  568.1109,  196.2637],\n",
    "        [ 321.0241,  605.9488,  547.1797,  936.0553],\n",
    "        [ 428.1929,   38.3956,  554.8650,  199.8660],\n",
    "        [ 746.6036,  615.0887,  971.1607,  935.0892],\n",
    "        [1102.4276,  207.2869, 1372.0725,  242.2401],\n",
    "        [1072.1843,  268.3727, 1380.2273,  940.6075],\n",
    "        [ 946.0789,  212.6116, 1191.3138,  244.1134],\n",
    "        [ 855.3904,  214.8240, 1148.2393,  244.2358],\n",
    "        [ 526.0251,   84.6113,  956.9733,  159.2933],\n",
    "        [1602.9233, 1065.2534, 1829.8510, 1178.2430],\n",
    "        [-242.1230, -158.3021,  591.4835,  859.7520],\n",
    "        [1035.1334,   24.6038, 1912.5560, 1176.9390],\n",
    "        [ 335.6111,  284.0186,  677.6199,  943.9149],\n",
    "        [ 135.1845,  549.4729, 1398.5144, 1165.3425],\n",
    "        [-271.7565, -259.6675,  620.7223,  518.3907],\n",
    "        [ 266.8770,  979.6051, 1503.4895, 1183.5458],\n",
    "        [ 413.4843,   28.3230, 1088.3103,  201.7365],\n",
    "        [ 120.5829,  922.5416, 1641.0110, 1187.8499],\n",
    "        [1040.3624,  269.0762, 1410.5273,  947.1584],\n",
    "        [ 336.1257,  986.8702, 1197.7725, 1164.8531],\n",
    "        [ 771.3053,  614.1293,  871.9078,  646.2388],\n",
    "        [ 808.9615,  917.7413, 1042.6924,  969.2343],\n",
    "        [ 361.3281,  288.9001,  601.4477,  612.9789],\n",
    "        [1798.4371,  -47.2978, 2046.5907,   95.5865],\n",
    "        [ 346.3434,  216.3328,  481.6689,  238.2121],\n",
    "        [ 605.1952,  703.0776, 1852.5463, 1195.7296],\n",
    "        [ 346.3434,  216.3328,  481.6689,  238.2121],\n",
    "        [ 559.0224,   68.1338, 1055.6260,  148.4330],\n",
    "        [1664.7755, 1051.6240, 1876.0095, 1191.6969],\n",
    "        [ 214.1127,  209.4996, 1016.6656, 1164.9806],\n",
    "        [ 511.7601,   85.6599, 1151.7854,  191.8203],\n",
    "        [1834.2233, 1116.4379, 1945.0957, 1196.0771],\n",
    "        [-205.5402, -105.0520,  916.1711,  466.4155],\n",
    "        [ 254.4914, 1122.7365,  511.3465, 1189.0867],\n",
    "        [ 257.3645,  217.7309, 1414.2401,  250.7157],\n",
    "        [1281.4097,   21.8314, 1939.2560, 1181.8925],\n",
    "        [1331.4860, -106.3575, 2310.1462,  391.7843],\n",
    "        [1646.0857,   41.0714, 1847.3046,  135.3579],\n",
    "        [ 356.8601,  865.6786,  858.8425, 1001.6323],\n",
    "        [1797.8419,  112.0865, 1922.6658,  372.2739],\n",
    "        [ 511.7601,   85.6599, 1151.7854,  191.8203],\n",
    "        [  21.5554,  859.8798, 1493.3877, 1180.0155],\n",
    "        [ 263.6172,  212.8943, 1442.7341,  240.9092],\n",
    "        [ 516.8245,   82.9084,  857.6564,  198.6778],\n",
    "        [1579.3196,  865.5213, 1845.1697, 1170.8385],\n",
    "        [ 998.0509,  -69.6900, 2263.0579,  420.4548],\n",
    "        [1000.9614,  593.9443, 1878.0334, 1170.8518],\n",
    "        [ 875.6599, 1072.2188, 1000.1871, 1112.0150],\n",
    "        [ 303.8547,  262.9725,  969.4770,  947.5837],\n",
    "        [  52.9637,  755.6959, 1336.9678, 1213.2312],\n",
    "        [ 998.0509,  -69.6900, 2263.0579,  420.4548],\n",
    "        [ -46.3515,  570.4091,  991.9885, 1162.0546]], device='cuda:0')}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export predictions of new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeJson(jsonObject, filename):\n",
    "    if filename.split(\".\")[-1] != \"json\":\n",
    "        filename = f\"{filename}.json\"\n",
    "    with open(filename, \"w\") as g:\n",
    "        g.write(jsonObject)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path_new_images = \"../new_images/\"\n",
    "path_predictions = \"../predicted_images\"\n",
    "input_file = \"coco_new.json\"\n",
    "pred_coco_filename = \"coco_predictions.json\"\n",
    "threshold_postprocessing = 0.2 # zu 50% sicher ist sich das modell, dass dies ein schaden ist\n",
    "\n",
    "list_paths = [os.path.join(path_new_images, filename) for filename in os.listdir(path_new_images) if\n",
    "                            filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "\n",
    "processor = DetaImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "new_dataset = CocoDetection(img_folder=path_new_images, processor=processor, image_name=input_file)\n",
    "\n",
    "print(\"Number of training examples:\", len(new_dataset))\n",
    "\n",
    "# Copy json if not in folder else create\n",
    "pred_path = os.path.join(path_predictions, pred_coco_filename)\n",
    "\n",
    "with open(os.path.join(path_new_images, input_file), \"r\") as f:\n",
    "    coco_tmp = json.load(f)\n",
    "\n",
    "if os.path.exists(pred_path):\n",
    "    with open(pred_path, \"r\") as f:\n",
    "        coco_predict = json.load(f)\n",
    "    \n",
    "    # append image infos\n",
    "    for x in coco_tmp[\"images\"]:\n",
    "        tmp = True\n",
    "        for z in coco_predict[\"images\"]:\n",
    "            if x[\"id\"] == z[\"id\"]:\n",
    "                tmp = False\n",
    "        if tmp:\n",
    "            coco_predict[\"images\"].append(x)\n",
    "\n",
    "else:\n",
    "    coco_predict = copy.copy(coco_tmp)\n",
    "\n",
    "# start id for annotations\n",
    "if coco_predict[\"annotations\"] == []:\n",
    "    id_count = 0\n",
    "else:\n",
    "    id_count = coco_predict[\"annotations\"][-1][\"id\"] + 1\n",
    "\n",
    "for z in range(len(list_paths)):\n",
    "    pixel_values, target = new_dataset[z]\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward pass to get class logits and bounding boxes\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "    \n",
    "    # postprocess model outputs\n",
    "    width, height = target[\"orig_size\"]\n",
    "    p_postprocessed_outputs = processor.post_process_object_detection(outputs,\n",
    "                                                                    target_sizes=[(height, width)],\n",
    "                                                                    threshold=threshold_postprocessing)\n",
    "    # darf nur einmal durchlaufen, sonst gibts doppelte einträge\n",
    "    for d in range(len(p_postprocessed_outputs[0][\"labels\"])):\n",
    "        # Write predictions into annotations from pred_coco_filename\n",
    "        # boxes == [xmin, ymin, xmax, ymax]\n",
    "        box = p_postprocessed_outputs[0][\"boxes\"][d].tolist()\n",
    "        dict_new_pred = {\"id\":id_count, \n",
    "                         \"image_id\":torch.Tensor.item(target[\"image_id\"][0]), \n",
    "                         \"category_id\":int(p_postprocessed_outputs[0][\"labels\"][d]),\n",
    "                         \"segmentation\":[], \n",
    "                         \"bbox\":p_postprocessed_outputs[0][\"boxes\"][d].tolist(), \n",
    "                         \"ignore\":0, \n",
    "                         \"iscrowd\":0, \n",
    "                         \"area\":((box[2]-box[0])*(box[3]-box[1]))}\n",
    "        coco_predict[\"annotations\"].append(dict_new_pred)\n",
    "        id_count += 1\n",
    "\n",
    "    for i in range(len(coco_tmp[\"images\"])):\n",
    "        if coco_tmp[\"images\"][i][\"id\"] == torch.Tensor.item(target[\"image_id\"][0]):\n",
    "            del coco_tmp[\"images\"][i]\n",
    "    # move image to new location\n",
    "    shutil.move(list_paths[z], os.path.join(path_predictions, list_paths[z].split(\"/\")[-1]))\n",
    "\n",
    "\n",
    "# uncomment to run\n",
    "# writeJson(json.dumps(coco_tmp, indent=4), os.path.join(path_new_images, input_file))\n",
    "# writeJson(json.dumps(coco_predict, indent=4), pred_path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
