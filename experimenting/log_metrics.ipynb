{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': tensor(0.6000), 'map_50': tensor(1.), 'map_75': tensor(1.), 'map_small': tensor(-1.), 'map_medium': tensor(-1.), 'map_large': tensor(0.6000), 'mar_1': tensor(0.6000), 'mar_10': tensor(0.6000), 'mar_100': tensor(0.6000), 'mar_small': tensor(-1.), 'mar_medium': tensor(-1.), 'mar_large': tensor(0.6000), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(0, dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "from torch import BoolTensor, IntTensor, Tensor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# Preds should be a list of elements, where each element is a dict\n",
    "# containing 3 keys: boxes, scores, labels\n",
    "mask_pred = [\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "]\n",
    "preds = [\n",
    "    {\n",
    "        # The boxes keyword should contain an [N,4] tensor,\n",
    "        # where N is the number of detected boxes with boxes of the format\n",
    "        # [xmin, ymin, xmax, ymax] in absolute image coordinates\n",
    "        \"boxes\": Tensor([[258.0, 41.0, 606.0, 285.0]]),\n",
    "        # The scores keyword should contain an [N,] tensor where\n",
    "        # each element is confidence score between 0 and 1\n",
    "        \"scores\": Tensor([0.536]),\n",
    "        # The labels keyword should contain an [N,] tensor\n",
    "        # with integers of the predicted classes\n",
    "        \"labels\": IntTensor([0]),\n",
    "        # The masks keyword should contain an [N,H,W] tensor,\n",
    "        # where H and W are the image height and width, respectively,\n",
    "        # with boolean masks. This is only required when iou_type is `segm`.\n",
    "        \"masks\": BoolTensor([mask_pred]),\n",
    "    }\n",
    "]\n",
    "\n",
    "# Target should be a list of elements, where each element is a dict\n",
    "# containing 2 keys: boxes and labels (and masks, if iou_type is `segm`).\n",
    "# Each keyword should be formatted similar to the preds argument.\n",
    "# The number of elements in preds and target need to match\n",
    "mask_tgt = [\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "]\n",
    "target = [\n",
    "    {\n",
    "        \"boxes\": Tensor([[214.0, 41.0, 562.0, 285.0]]),\n",
    "        \"labels\": IntTensor([0]),\n",
    "        \"masks\": BoolTensor([mask_tgt]),\n",
    "    }\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize metric\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "\n",
    "    # Update metric with predictions and respective ground truth\n",
    "    metric.update(preds, target)\n",
    "\n",
    "    # Compute the results\n",
    "    result = metric.compute()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         # we specify the \"no_timm\" variant here to not rely on the timm library\n",
    "         # for the convolutional backbone\n",
    "         self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n",
    "                                                             revision=\"no_timm\",\n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "         self.save_hyperparameters()\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "\n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "       print(outputs)\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
