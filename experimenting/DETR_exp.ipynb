{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR\n",
    "\n",
    "## Version 0.4\n",
    "\n",
    "Experiment mit standard DETR und export predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, torchvision, torchaudio\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "import json\n",
    "import copy\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from transformers import AutoImageProcessor, DetrForObjectDetection\n",
    "from transformers import DetrConfig, DetrImageProcessor\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, processor, image_name):\n",
    "        ann_file = os.path.join(img_folder, image_name)\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        # feel free to add data augmentation here before passing them to the next step\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create Train and Validation Dataset\n",
    "  #Minimal Dataset\n",
    "# train_dataset = CocoDetection(img_folder='../../../old/BAA/Data/train', processor=processor, image_name=\"coco_train.json\")\n",
    "# val_dataset = CocoDetection(img_folder='../../../old/BAA/Data/val', processor=processor, image_name=\"coco_val.json\")\n",
    "\n",
    "  #Maximal Dataset\n",
    "train_dataset = CocoDetection(img_folder='../../../old/BAA/Data/train_max', processor=processor, image_name=\"coco_train_max.json\")\n",
    "val_dataset = CocoDetection(img_folder='../../../old/BAA/Data/val_max', processor=processor, image_name=\"coco_val_max.json\")\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2)\n",
    "batch = next(iter(train_dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "\t\tdef __init__(self, lr, lr_backbone, weight_decay):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\t# replace COCO classification head with custom head\n",
    "\t\t\t# we specify the \"no_timm\" variant here to not rely on the timm library\n",
    "\t\t\t# for the convolutional backbone\n",
    "\t\t\tself.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trevision=\"no_timm\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnum_labels=len(id2label),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tignore_mismatched_sizes=True)\n",
    "\t\t\t# see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "\t\t\tself.processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\t\t\tself.cmetric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "\t\t\tself.val_epoch_count = 0 # to not log at the first run\n",
    "\t\t\tself.save_hyperparameters()\n",
    "\t\t\tself.lr = lr\n",
    "\t\t\tself.lr_backbone = lr_backbone\n",
    "\t\t\tself.weight_decay = weight_decay\n",
    "\t\t\tself.training_step_pred = []\n",
    "\t\t\tself.training_step_targ = []\n",
    "\t\t\tself.validation_step_pred = []\n",
    "\t\t\tself.validation_step_targ = []\n",
    "\n",
    "\t\tdef forward(self, pixel_values, pixel_mask):\n",
    "\t\t\toutputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "\t\t\treturn outputs\n",
    "\n",
    "\t\tdef common_step(self, batch, batch_idx, t_v):\n",
    "\t\t\tpixel_values = batch[\"pixel_values\"]\n",
    "\t\t\tpixel_mask = batch[\"pixel_mask\"]\n",
    "\t\t\tlabels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\t\t\toutputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss_dict = outputs.loss_dict\n",
    "\t\t\t\n",
    "\t\t\tif t_v:\n",
    "\t\t\t\tself.data_prep_train(pixel_values, pixel_mask, labels)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.data_prep_val(pixel_values, pixel_mask, labels)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\treturn loss, loss_dict\n",
    "\n",
    "\t\tdef training_step(self, batch, batch_idx):\n",
    "\t\t\tloss, loss_dict = self.common_step(batch, batch_idx, True)\n",
    "\t\t\t# logs metrics for each training_step,\n",
    "\t\t\t# and the average across the epoch\n",
    "\t\t\tself.log(\"training_loss\", loss)\n",
    "\t\t\tfor k,v in loss_dict.items():\n",
    "\t\t\t\tself.log(\"train_\" + k, v.item())\n",
    "\t\t\treturn loss\n",
    "\n",
    "\t\tdef on_train_epoch_end(self) -> None:\n",
    "\t\t\tprint(\"onTrainEpoch\")\n",
    "\t\t\tt1 = time.time()\n",
    "\t\t\tself.cmetric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "\t\t\tself.cmetric.update(self.training_step_pred, self.training_step_targ)\n",
    "\t\t\tresult = self.cmetric.compute()\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tfor k, v in result.items():\n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\t\tif counter < 15:\n",
    "\t\t\t\t\twandb.log({\"train_epoch_\" + k: v.item()})\n",
    "\t\t\t\t\tprint({\"train_\" + k: v.item()})\n",
    "\t\t\tself.training_step_pred = []\n",
    "\t\t\tself.training_step_targ = []\n",
    "\t\t\tt2 = time.time()\n",
    "\t\t\tprint(round(t2 - t1, 4))\n",
    "\t\t\t\n",
    "\t\t# temp\n",
    "\t\t\"\"\"def on_train_end(self) -> None:\n",
    "\t\t\tprint(\"onTrainEnd\")\n",
    "\t\t\tt1 = time.time()\n",
    "\t\t\tself.cmetric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "\t\t\tself.cmetric.update(self.training_step_pred, self.training_step_targ)\n",
    "\t\t\tresult = self.cmetric.compute()\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tfor k, v in result.items():\n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\t\tif counter < 15:\n",
    "\t\t\t\t\twandb.log({\"train_\" + k: v.item()})\n",
    "\t\t\tself.training_step_pred = []\n",
    "\t\t\tself.training_step_targ = []\n",
    "\t\t\tt2 = time.time()\n",
    "\t\t\tprint(round(t2 - t1, 4))\"\"\"\n",
    "\n",
    "\t\tdef validation_step(self, batch, batch_idx):\n",
    "\t\t\tloss, loss_dict = self.common_step(batch, batch_idx, False)\n",
    "\t\t\tself.log(\"validation_loss\", loss)\n",
    "\t\t\tfor k,v in loss_dict.items():\n",
    "\t\t\t\tself.log(\"validation_\" + k, v.item())\n",
    "\n",
    "\t\t\treturn loss\n",
    "\t\t\n",
    "\t\tdef on_validation_epoch_end(self) -> None:\n",
    "\t\t\tif self.val_epoch_count == 1:\n",
    "\t\t\t\tprint(\"onValEpoch\")\n",
    "\t\t\t\tt1 = time.time()\n",
    "\t\t\t\tself.cmetric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "\t\t\t\tself.cmetric.update(self.training_step_pred, self.training_step_targ)\n",
    "\t\t\t\tresult = self.cmetric.compute()\n",
    "\t\t\t\tcounter = 0\n",
    "\t\t\t\tfor k, v in result.items():\n",
    "\t\t\t\t\tcounter += 1\n",
    "\t\t\t\t\tif counter < 15:\n",
    "\t\t\t\t\t\twandb.log({\"validation_epoch_\" + k: v.item()})\n",
    "\t\t\t\t\t\tprint({\"validation_epoch_\" + k: v.item()})\n",
    "\t\t\t\tself.validation_step_pred = []\n",
    "\t\t\t\tself.validation_step_targ = []\n",
    "\t\t\t\tt2 = time.time()\n",
    "\t\t\t\tprint(round(t2 - t1, 4))\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.val_epoch_count = 1\n",
    "\n",
    "\t\t\t\n",
    "\t\t#temp\n",
    "\t\t\"\"\"def on_validation_end(self) -> None:\n",
    "\t\t\tprint(\"onValEnd\")\n",
    "\t\t\tt1 = time.time()\n",
    "\t\t\tself.cmetric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "\t\t\tself.cmetric.update(self.training_step_pred, self.training_step_targ)\n",
    "\t\t\tresult = self.cmetric.compute()\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tfor k, v in result.items():\n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\t\tif counter < 15:\n",
    "\t\t\t\t\twandb.log({\"validation_\" + k: v.item()})\n",
    "\t\t\tself.validation_step_pred = []\n",
    "\t\t\tself.validation_step_targ = []\n",
    "\t\t\tt2 = time.time()\n",
    "\t\t\tprint(round(t2 - t1, 4))\"\"\"\n",
    "\n",
    "\t\tdef configure_optimizers(self):\n",
    "\t\t\tparam_dicts = [\n",
    "\t\t\t\t{\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "\t\t\t\t\t\"lr\": self.lr_backbone,\n",
    "\t\t\t\t},\n",
    "\t\t\t]\n",
    "\t\t\toptimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "\t\t\t\t\t\t\t\t\tweight_decay=self.weight_decay)\n",
    "\n",
    "\t\t\treturn optimizer\n",
    "\n",
    "\t\tdef train_dataloader(self):\n",
    "\t\t\treturn train_dataloader\n",
    "\n",
    "\t\tdef val_dataloader(self):\n",
    "\t\t\treturn val_dataloader\n",
    "\t\t\n",
    "\t\t# EVALUATION\n",
    "\n",
    "\t\tdef convert_to_xywh(self, boxes):\n",
    "\t\t\txmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "\t\t\treturn torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "\t\tdef prepare_for_coco_detection(self, predictions):\n",
    "\t\t\t#used for the evaluation \n",
    "\t\t\tcoco_results = []\n",
    "\t\t\tfor original_id, prediction in predictions.items():\n",
    "\t\t\t\tif len(prediction) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tboxes = prediction[\"boxes\"]\n",
    "\t\t\t\tboxes = self.convert_to_xywh(boxes).tolist()\n",
    "\t\t\t\tscores = prediction[\"scores\"].tolist()\n",
    "\t\t\t\tlabels = prediction[\"labels\"].tolist()\n",
    "\n",
    "\t\t\t\tcoco_results.extend(\n",
    "\t\t\t\t\t[\n",
    "\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\t\"image_id\": original_id,\n",
    "\t\t\t\t\t\t\t\"category_id\": labels[k],\n",
    "\t\t\t\t\t\t\t\"bbox\": box,\n",
    "\t\t\t\t\t\t\t\"score\": scores[k],\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\tfor k, box in enumerate(boxes)\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t)\n",
    "\t\t\treturn coco_results\n",
    "\t\t\n",
    "\t\tdef data_prep_train(self, pixel_values, pixel_mask, labels):\n",
    "\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\toutputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "\t\t\torig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "\t\t\tresults = self.processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n",
    "\t\t\tfor n in results:\n",
    "\t\t\t\tself.training_step_pred.append(n)\n",
    "\t\t\tfor i in labels:\n",
    "\t\t\t\tself.training_step_targ.append({\"boxes\":i[\"boxes\"], \"labels\":i[\"class_labels\"]})\n",
    "\n",
    "\t\tdef data_prep_val(self, pixel_values, pixel_mask, labels):\n",
    "\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\toutputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "\t\t\torig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "\t\t\tresults = self.processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n",
    "\t\t\tfor n in results:\n",
    "\t\t\t\tself.validation_step_pred.append(n)\n",
    "\t\t\tfor i in labels:\n",
    "\t\t\t\tself.validation_step_targ.append({\"boxes\":i[\"boxes\"], \"labels\":i[\"class_labels\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# login to weights and biases, to relogin: wandb.login(key=\"YOUR KEY\", relogin=True)\n",
    "wandb.login()\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "run_name = \"Run at the {}\".format(date)\n",
    "\n",
    "lr = 1e-4\n",
    "lr_backbone=1e-5\n",
    "weight_decay=1e-4\n",
    "\n",
    "project = \"BAA_Book_Damage_Detection\"\n",
    "\n",
    "wandb.init(\n",
    "    project=project,\n",
    "    name=run_name,\n",
    "    config={\"learning_rate_transformer\": lr,\n",
    "            \"learning_rate_backbone\": lr_backbone,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"architecture\": \"DETR\",\n",
    "            \"dataset\": \"test_minimal_set\",\n",
    "            \"train_set_size\": len(train_dataset)}\n",
    "    )\n",
    "\n",
    "model = Detr(lr=lr, lr_backbone=lr_backbone, weight_decay=weight_decay)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "outputs = model(pixel_values=batch['pixel_values'].to(device), pixel_mask=batch['pixel_mask'].to(device))\n",
    "print(outputs.logits.shape)\n",
    "\n",
    "max_steps_var = 460\n",
    "\n",
    "wandb_logger = WandbLogger(\"Optimize_Logging\", \"../models/\", project=project, log_model=True, checkpoint_name=f\"DETR_{max_steps_var}_Steps\")\n",
    "trainer = Trainer(max_steps=max_steps_var, gradient_clip_val=0.1, logger=wandb_logger)\n",
    "trainer.fit(model)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the checkpoint\n",
    "\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('damaged-books-detection-ml/BAA_Book_Damage_Detection/optimized_logs:v1', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "model = Detr.load_from_checkpoint(artifact_dir + \"/model.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "def prepare_for_coco_detection(predictions):\n",
    "    coco_results = []\n",
    "    for original_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = convert_to_xywh(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        coco_results.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": labels[k],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": scores[k],\n",
    "                }\n",
    "                for k, box in enumerate(boxes)\n",
    "            ]\n",
    "        )\n",
    "    return coco_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initialize evaluator with ground truth (gt)\n",
    "evaluator = CocoEvaluator(coco_gt=val_dataset.coco, iou_types=[\"bbox\"])\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "      \n",
    "\n",
    "    # turn into a list of dictionaries (one item for each example in the batch)\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n",
    "\n",
    "    # provide to metric\n",
    "    # metric expects a list of dictionaries, each item\n",
    "    # containing image_id, category_id, bbox and score keys\n",
    "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    predictions = prepare_for_coco_detection(predictions)\n",
    "    evaluator.update(predictions)\n",
    "\n",
    "evaluator.synchronize_between_processes()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()\n",
    "print(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "for z in range(6):\n",
    "    pixel_values, target = val_dataset[z]\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    annotations = val_dataset.coco.imgToAnns[\"image_id\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward pass to get class logits and bounding boxes\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "\n",
    "    # colors for visualization\n",
    "    COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "            [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "    def plot_results(pil_img, scores, labels, boxes, anno):\n",
    "        plt.figure(figsize=(16,10))\n",
    "        plt.imshow(pil_img)\n",
    "        ax = plt.gca()\n",
    "        colors = COLORS * 100\n",
    "        types = {}\n",
    "        for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                fill=False, color=c, linewidth=3))\n",
    "            text = f'{id2label[label]}: {score:0.2f}'\n",
    "            textx = text.split(\": \")\n",
    "            if textx[0] in types:\n",
    "                types.update({textx[0]: types[textx[0]] + 1})\n",
    "            else: \n",
    "                types.update({textx[0]:1})\n",
    "            ax.text(xmin, ymin, text, fontsize=0,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.0))\n",
    "        print(types)\n",
    "        \n",
    "        # draw annotatet boxes\n",
    "        typesAnno = {}\n",
    "        color = [0.000, 0.000, 0.000]\n",
    "        # TODO Resize bbox\n",
    "        for annot in anno:\n",
    "            (x, y, w, h) = annot[\"bbox\"]\n",
    "            (xmin, ymin, xmax, ymax) = (x, y, x + w, y + h)\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                fill=False, color=color, linewidth=3))\n",
    "            text = f'{id2label[annot[\"category_id\"]]}'\n",
    "            if text in typesAnno:\n",
    "                typesAnno.update({text: typesAnno[text] + 1})\n",
    "            else: \n",
    "                typesAnno.update({text:1})\n",
    "            ax.text(xmin, ymin, text, fontsize=0,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.0))\n",
    "        print(typesAnno)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # load image based on ID\n",
    "    image_id = target['image_id'].item()\n",
    "    image = val_dataset.coco.loadImgs(image_id)[0]\n",
    "    anno = val_dataset.coco.imgToAnns[image_id]\n",
    "    image = Image.open(os.path.join('../../../old/BAA/Data/val_max', image['file_name']))\n",
    "\n",
    "    # postprocess model outputs\n",
    "    width, height = image.size\n",
    "    postprocessed_outputs = processor.post_process_object_detection(outputs,\n",
    "                                                                    target_sizes=[(height, width)],\n",
    "                                                                    threshold=0.5)\n",
    "    results = postprocessed_outputs[0]\n",
    "    print(results)\n",
    "    plot_results(image, results['scores'], results['labels'], results['boxes'], anno)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export predictions of new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target = val_dataset[0]\n",
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeJson(jsonObject, filename):\n",
    "    if filename.split(\".\")[-1] != \"json\":\n",
    "        filename = f\"{filename}.json\"\n",
    "    with open(filename, \"w\") as g:\n",
    "        g.write(jsonObject)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path_new_images = \"../new_images/\"\n",
    "path_predictions = \"../predicted_images\"\n",
    "input_file = \"coco_new.json\"\n",
    "pred_coco_filename = \"coco_predictions.json\"\n",
    "threshold_postprocessing = 0.5 # zu 50% sicher ist sich das modell, dass dies ein schaden ist\n",
    "\n",
    "list_paths = [os.path.join(path_new_images, filename) for filename in os.listdir(path_new_images) if\n",
    "                            filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "new_dataset = CocoDetection(img_folder=path_new_images, processor=processor, image_name=input_file)\n",
    "\n",
    "print(\"Number of training examples:\", len(new_dataset))\n",
    "\n",
    "# Copy json if not in folder else create\n",
    "pred_path = os.path.join(path_predictions, pred_coco_filename)\n",
    "\n",
    "with open(os.path.join(path_new_images, input_file), \"r\") as f:\n",
    "    coco_tmp = json.load(f)\n",
    "\n",
    "if os.path.exists(pred_path):\n",
    "    with open(pred_path, \"r\") as f:\n",
    "        coco_predict = json.load(f)\n",
    "    \n",
    "    # append image infos\n",
    "    for x in coco_tmp[\"images\"]:\n",
    "        tmp = True\n",
    "        for z in coco_predict[\"images\"]:\n",
    "            if x[\"id\"] == z[\"id\"]:\n",
    "                tmp = False\n",
    "        if tmp:\n",
    "            coco_predict[\"images\"].append(x)\n",
    "\n",
    "else:\n",
    "    coco_predict = copy.copy(coco_tmp)\n",
    "\n",
    "# start id for annotations\n",
    "if coco_predict[\"annotations\"] == []:\n",
    "    id_count = 0\n",
    "else:\n",
    "    id_count = coco_predict[\"annotations\"][-1][\"id\"] + 1\n",
    "\n",
    "for z in range(len(list_paths)):\n",
    "    pixel_values, target = new_dataset[z]\n",
    "    pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward pass to get class logits and bounding boxes\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "    \n",
    "    # postprocess model outputs\n",
    "    width, height = target[\"size\"]\n",
    "    p_postprocessed_outputs = processor.post_process_object_detection(outputs,\n",
    "                                                                    target_sizes=[(height, width)],\n",
    "                                                                    threshold=threshold_postprocessing)\n",
    "    # darf nur einmal durchlaufen, sonst gibts doppelte einträge\n",
    "    for d in range(len(p_postprocessed_outputs[0][\"labels\"])):\n",
    "        # Write predictions into annotations from pred_coco_filename\n",
    "        # boxes == [xmin, ymin, xmax, ymax]\n",
    "        box = p_postprocessed_outputs[0][\"boxes\"][d].tolist()\n",
    "        dict_new_pred = {\"id\":id_count, \n",
    "                         \"image_id\":torch.Tensor.item(target[\"image_id\"][0]), \n",
    "                         \"category_id\":int(p_postprocessed_outputs[0][\"labels\"][d]),\n",
    "                         \"segmentation\":[], \n",
    "                         \"bbox\":p_postprocessed_outputs[0][\"boxes\"][d].tolist(), \n",
    "                         \"ignore\":0, \n",
    "                         \"iscrowd\":0, \n",
    "                         \"area\":((box[2]-box[0])*(box[3]-box[1]))}\n",
    "        coco_predict[\"annotations\"].append(dict_new_pred)\n",
    "        id_count += 1\n",
    "\n",
    "    for i in range(len(coco_tmp[\"images\"])):\n",
    "        if coco_tmp[\"images\"][i][\"id\"] == torch.Tensor.item(target[\"image_id\"][0]):\n",
    "            del coco_tmp[\"images\"][i]\n",
    "    # move image to new location\n",
    "    shutil.move(list_paths[z], os.path.join(path_predictions, list_paths[z].split(\"/\")[-1]))\n",
    "\n",
    "\n",
    "\n",
    "writeJson(json.dumps(coco_tmp, indent=4), os.path.join(path_new_images, input_file))\n",
    "writeJson(json.dumps(coco_predict, indent=4), pred_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamps Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "for x in range(9999999):\n",
    "    x**2\n",
    "t2 = time.time()\n",
    "\n",
    "print(round(t2 - t1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
